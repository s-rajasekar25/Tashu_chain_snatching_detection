{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13957334,"sourceType":"datasetVersion","datasetId":8896695},{"sourceId":13992317,"sourceType":"datasetVersion","datasetId":8917545},{"sourceId":3012996,"sourceType":"kernelVersion"},{"sourceId":130163259,"sourceType":"kernelVersion"},{"sourceId":224123514,"sourceType":"kernelVersion"},{"sourceId":283964835,"sourceType":"kernelVersion"},{"sourceId":288027561,"sourceType":"kernelVersion"},{"sourceId":695356,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":527360,"modelId":541406},{"sourceId":695361,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":527365,"modelId":541411},{"sourceId":696513,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":528289,"modelId":542336}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nimport os\n\n# --- 1. CONFIGURATION ---\nYOLO_PATH = '/kaggle/input/yolo-v8/other/default/1/yolov8m.pt'\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx'\nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_1.mp4'\nOUTPUT_PATH = '/kaggle/working/output_snatching_skeleton.mp4'\n\n# Model Input Size (RTMPose standard)\nINPUT_H, INPUT_W = 256, 256\n\n# --- 2. LOAD MODELS ---\nprint(\"Loading YOLOv8...\")\nyolo_model = YOLO(YOLO_PATH)\n\nprint(\"Loading RTMPose (AP-10K)...\")\ntry:\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=providers)\n    print(f\"RTMPose running on: {ort_session.get_providers()[0]}\")\nexcept Exception as e:\n    print(f\"CUDA Error ({e}). Running on CPU.\")\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\n\ninput_name = ort_session.get_inputs()[0].name\n\n# --- 3. HELPER FUNCTIONS ---\n\ndef preprocess_for_rtmpose(img_crop, h, w):\n    # Resize, Normalize, Transpose (HWC -> BCHW)\n    resized = cv2.resize(img_crop, (w, h))\n    mean = np.array([123.675, 116.28, 103.53])\n    std = np.array([58.395, 57.12, 57.375])\n    img_data = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_data = (img_data - mean) / std\n    img_data = img_data.astype(np.float32).transpose(2, 0, 1)\n    img_data = np.expand_dims(img_data, axis=0)\n    return img_data\n\ndef decode_simcc_output(outputs, crop_h, crop_w):\n    \"\"\"\n    Decodes SimCC output from RTMPose.\n    Returns: Array of (x, y) coordinates for each keypoint relative to the CROP.\n    \"\"\"\n    # RTMPose usually returns 2 outputs: simcc_x and simcc_y\n    # Shape is usually (Batch, Num_Points, Bins)\n    simcc_x, simcc_y = outputs[0], outputs[1]\n    \n    # 1. Find the index with the highest score (Argmax)\n    x_locs = np.argmax(simcc_x, axis=2)[0]\n    y_locs = np.argmax(simcc_y, axis=2)[0]\n    \n    # 2. Scale these indices back to the crop size\n    # The model output bins usually correspond to the Input Size (256) * 2\n    # But strictly, we just map 0..Bins to 0..CropSize\n    \n    num_bins_x = simcc_x.shape[2]\n    num_bins_y = simcc_y.shape[2]\n    \n    # Map model coordinates (0..256 usually) to Crop Coordinates (0..crop_w)\n    # We assume the bins represent the range [0, 1] in normalized space\n    x_locs = x_locs / (num_bins_x) * crop_w\n    y_locs = y_locs / (num_bins_y) * crop_h\n    \n    # Combine into (K, 2) array\n    keypoints = np.stack([x_locs, y_locs], axis=-1)\n    return keypoints\n\n# --- 4. PROCESSING LOOP ---\nif not os.path.exists(VIDEO_PATH):\n    print(f\"Error: Video not found at {VIDEO_PATH}\")\n    exit()\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps != fps or fps <= 0: fps = 25\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(fps), (width, height))\n\nprint(f\"Processing {width}x{height} @ {fps}fps...\")\n\nframe_idx = 0\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    if frame is None or frame.size == 0: continue\n\n    frame_idx += 1\n    if frame_idx % 20 == 0:\n        print(f\"Processing frame {frame_idx}/{total_frames}...\")\n\n    # 1. Detect Person\n    results = yolo_model.predict(frame, classes=0, verbose=False, conf=0.5)\n\n    for result in results:\n        boxes = result.boxes\n        for box in boxes:\n            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n            \n            # Draw Box\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            \n            # 2. Crop for Pose\n            x1_c, y1_c = max(0, x1), max(0, y1)\n            x2_c, y2_c = min(width, x2), min(height, y2)\n            person_crop = frame[y1_c:y2_c, x1_c:x2_c]\n            \n            if person_crop.size > 0:\n                crop_h, crop_w = person_crop.shape[:2]\n                \n                # Preprocess\n                onnx_input = preprocess_for_rtmpose(person_crop, INPUT_H, INPUT_W)\n                \n                # Run Inference\n                outputs = ort_session.run(None, {input_name: onnx_input})\n                \n                # Decode Keypoints (SimCC)\n                # We check if output looks like SimCC (list of 2 arrays)\n                if len(outputs) >= 2:\n                    keypoints = decode_simcc_output(outputs, crop_h, crop_w)\n                    \n                    # Draw Keypoints on the MAIN frame\n                    for i, (kp_x, kp_y) in enumerate(keypoints):\n                        # Add the crop offset (x1, y1) to map back to full image\n                        real_x = int(kp_x + x1_c)\n                        real_y = int(kp_y + y1_c)\n                        \n                        # Draw Red Dot\n                        cv2.circle(frame, (real_x, real_y), 3, (0, 0, 255), -1)\n                else:\n                    # Fallback for some export types that return just 1 tensor\n                    cv2.putText(frame, \"Format Error\", (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)\n\n    out.write(frame)\n\ncap.release()\nout.release()\nprint(f\"DONE. Video saved to: {OUTPUT_PATH}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nimport os\n\n# --- 1. CONFIGURATION ---\nYOLO_PATH = '/kaggle/input/yolo-v8/other/default/1/yolov8m.pt'\n# NOTE: This should ideally be a Human Pose model (COCO trained) for correct indices\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx' \nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_1.mp4'\nOUTPUT_PATH = '/kaggle/working/output_neck_zone.mp4'\n\nINPUT_H, INPUT_W = 256, 256\n\n# --- COCO KEYPOINT INDICES (Standard for Human Pose) ---\n# We use these to find the neck and torso.\nKEYPOINT_L_SHOULDER = 5\nKEYPOINT_R_SHOULDER = 6\nKEYPOINT_L_HIP = 11\nKEYPOINT_R_HIP = 12\n\n# --- 2. LOAD MODELS ---\nprint(\"Loading YOLOv8...\")\nyolo_model = YOLO(YOLO_PATH)\n\nprint(\"Loading RTMPose...\")\ntry:\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=providers)\n    print(f\"RTMPose loaded on: {ort_session.get_providers()[0]}\")\nexcept Exception as e:\n    print(f\"CUDA Error ({e}). Using CPU.\")\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\n\ninput_name = ort_session.get_inputs()[0].name\n\n# --- 3. MATH FUNCTIONS (FROM YOUR DOCUMENT) ---\n\ndef get_distance(p1, p2):\n    \"\"\"Euclidean distance between two points (x,y).\"\"\"\n    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n\ndef calculate_neck_logic(keypoints):\n    \"\"\"\n    Implements Phase 2 Logic:\n    1. Neck Center = Midpoint of Shoulders (5 & 6)\n    2. Torso Length = Dist(Neck, Hips)\n    3. Zone Radius = 0.3 * Torso Length\n    \"\"\"\n    # We need at least up to index 12 (Right Hip) to calculate torso\n    if len(keypoints) <= 12:\n        return None, None\n\n    # Extract coordinates\n    l_shoulder = keypoints[KEYPOINT_L_SHOULDER]\n    r_shoulder = keypoints[KEYPOINT_R_SHOULDER]\n    l_hip = keypoints[KEYPOINT_L_HIP]\n    r_hip = keypoints[KEYPOINT_R_HIP]\n\n    # 1. Calculate Neck Center (Midpoint of Shoulders)\n    neck_x = (l_shoulder[0] + r_shoulder[0]) / 2\n    neck_y = (l_shoulder[1] + r_shoulder[1]) / 2\n    neck_center = (int(neck_x), int(neck_y))\n\n    # 2. Calculate Hip Center (Midpoint of Hips)\n    hip_x = (l_hip[0] + r_hip[0]) / 2\n    hip_y = (l_hip[1] + r_hip[1]) / 2\n    hip_center = (hip_x, hip_y)\n\n    # 3. Calculate Torso Length (Distance from Neck to Hips)\n    torso_length = get_distance(neck_center, hip_center)\n\n    # 4. Calculate Zone Radius (0.3 * Torso Length)\n    # Using max(10, ...) ensures the circle doesn't disappear if torso is tiny\n    radius = int(max(10, 0.3 * torso_length))\n\n    return neck_center, radius\n\ndef preprocess_for_rtmpose(img_crop, h, w):\n    resized = cv2.resize(img_crop, (w, h))\n    mean = np.array([123.675, 116.28, 103.53])\n    std = np.array([58.395, 57.12, 57.375])\n    img_data = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_data = (img_data - mean) / std\n    img_data = img_data.astype(np.float32).transpose(2, 0, 1)\n    img_data = np.expand_dims(img_data, axis=0)\n    return img_data\n\ndef decode_simcc_output(outputs, crop_h, crop_w):\n    simcc_x, simcc_y = outputs[0], outputs[1]\n    x_locs = np.argmax(simcc_x, axis=2)[0]\n    y_locs = np.argmax(simcc_y, axis=2)[0]\n    num_bins_x = simcc_x.shape[2]\n    num_bins_y = simcc_y.shape[2]\n    x_locs = x_locs / (num_bins_x) * crop_w\n    y_locs = y_locs / (num_bins_y) * crop_h\n    return np.stack([x_locs, y_locs], axis=-1)\n\n# --- 4. PROCESSING LOOP ---\nif not os.path.exists(VIDEO_PATH):\n    print(\"Video not found.\")\n    exit()\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps != fps or fps <= 0: fps = 25\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(fps), (width, height))\n\nprint(f\"Processing Logic: Neck Zone (Radius=0.3*Torso)...\")\n\nframe_idx = 0\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    if frame is None or frame.size == 0: continue\n\n    frame_idx += 1\n    if frame_idx % 20 == 0:\n        print(f\"Processing frame {frame_idx}/{total_frames}...\")\n\n    # 1. Detect Person\n    results = yolo_model.predict(frame, classes=0, verbose=False, conf=0.5)\n\n    for result in results:\n        boxes = result.boxes\n        for box in boxes:\n            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n            \n            # Crop Person\n            x1_c, y1_c = max(0, x1), max(0, y1)\n            x2_c, y2_c = min(width, x2), min(height, y2)\n            person_crop = frame[y1_c:y2_c, x1_c:x2_c]\n            \n            if person_crop.size > 0:\n                crop_h, crop_w = person_crop.shape[:2]\n                \n                # RTMPose Inference\n                onnx_input = preprocess_for_rtmpose(person_crop, INPUT_H, INPUT_W)\n                outputs = ort_session.run(None, {input_name: onnx_input})\n                \n                if len(outputs) >= 2:\n                    # Get Keypoints relative to CROP\n                    keypoints_crop = decode_simcc_output(outputs, crop_h, crop_w)\n                    \n                    # Convert to GLOBAL coordinates (Full Frame)\n                    keypoints_global = []\n                    for (kx, ky) in keypoints_crop:\n                        keypoints_global.append([kx + x1_c, ky + y1_c])\n                    \n                    # --- APPLY NECK ZONE LOGIC ---\n                    neck_center, neck_radius = calculate_neck_logic(keypoints_global)\n                    \n                    if neck_center is not None:\n                        # Draw the \"Neck Zone\" (Cyan Circle)\n                        cv2.circle(frame, neck_center, neck_radius, (255, 255, 0), 2)\n                        \n                        # Draw the Center Dot\n                        cv2.circle(frame, neck_center, 4, (0, 0, 255), -1)\n                        \n                        # Label it\n                        cv2.putText(frame, \"Neck Zone\", (neck_center[0] - 20, neck_center[1] - neck_radius - 5),\n                                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)\n\n    out.write(frame)\n\ncap.release()\nout.release()\nprint(f\"DONE. Output saved to: {OUTPUT_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nimport os\n\n# --- 1. CONFIGURATION ---\nYOLO_PATH = '/kaggle/input/yolo-v8/other/default/1/yolov8m.pt'\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx' \nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_1.mp4'\nOUTPUT_PATH = '/kaggle/working/output_neck_zone_fixed.mp4'\n\nINPUT_H, INPUT_W = 256, 256\n\n# --- 2. LOAD MODELS ---\nprint(\"Loading YOLOv8...\")\nyolo_model = YOLO(YOLO_PATH)\n\nprint(\"Loading RTMPose...\")\ntry:\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=providers)\nexcept Exception as e:\n    print(f\"CUDA Error ({e}). Using CPU.\")\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\n\ninput_name = ort_session.get_inputs()[0].name\n\n# --- 3. HELPER FUNCTIONS ---\n\ndef get_neck_from_box(x1, y1, x2, y2):\n    \"\"\"\n    FALLBACK: Calculates Neck Zone using the Bounding Box geometry.\n    Used because the Animal Pose model gives wrong keypoints for humans.\n    \"\"\"\n    box_h = y2 - y1\n    box_w = x2 - x1\n    \n    # 1. Neck Center: Horizontally centered, Vertically at top 20% of box\n    neck_x = x1 + (box_w / 2)\n    neck_y = y1 + (box_h * 0.20) \n    \n    # 2. Zone Radius: Scaled to box size (approx 15% of height)\n    radius = int(box_h * 0.15)\n    \n    return (int(neck_x), int(neck_y)), radius\n\ndef preprocess_for_rtmpose(img_crop, h, w):\n    resized = cv2.resize(img_crop, (w, h))\n    mean = np.array([123.675, 116.28, 103.53])\n    std = np.array([58.395, 57.12, 57.375])\n    img_data = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_data = (img_data - mean) / std\n    img_data = img_data.astype(np.float32).transpose(2, 0, 1)\n    img_data = np.expand_dims(img_data, axis=0)\n    return img_data\n\n# --- 4. PROCESSING LOOP ---\nif not os.path.exists(VIDEO_PATH):\n    print(\"Video not found.\")\n    exit()\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps != fps or fps <= 0: fps = 25\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(fps), (width, height))\n\nprint(f\"Processing with GEOMETRIC FIX for Neck Zone...\")\n\nframe_idx = 0\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    if frame is None or frame.size == 0: continue\n\n    frame_idx += 1\n    if frame_idx % 20 == 0:\n        print(f\"Processing frame {frame_idx}/{total_frames}...\")\n\n    # 1. Detect Person\n    results = yolo_model.predict(frame, classes=0, verbose=False, conf=0.5)\n\n    for result in results:\n        boxes = result.boxes\n        for box in boxes:\n            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n            \n            # --- FIX: USE BOX GEOMETRY FOR NECK ZONE ---\n            # This ignores the bad animal keypoints and calculates the neck \n            # based on where the human is standing in the box.\n            neck_center, neck_radius = get_neck_from_box(x1, y1, x2, y2)\n            \n            # Draw the Corrected Neck Zone (Cyan)\n            cv2.circle(frame, neck_center, neck_radius, (255, 255, 0), 2)\n            cv2.circle(frame, neck_center, 4, (0, 0, 255), -1)\n            cv2.putText(frame, \"Neck Zone\", (neck_center[0] - 20, neck_center[1] - neck_radius - 5),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)\n\n            # (Optional) Still run pose estimation if you want to see the dots\n            # But we don't use it for the circle anymore.\n            x1_c, y1_c = max(0, x1), max(0, y1)\n            x2_c, y2_c = min(width, x2), min(height, y2)\n            person_crop = frame[y1_c:y2_c, x1_c:x2_c]\n            \n            if person_crop.size > 0:\n                onnx_input = preprocess_for_rtmpose(person_crop, INPUT_H, INPUT_W)\n                outputs = ort_session.run(None, {input_name: onnx_input})\n                \n                # If you want to see the confused red dots, uncomment below:\n                # ... (drawing code omitted to keep visual clean) ...\n\n    out.write(frame)\n\ncap.release()\nout.release()\nprint(f\"DONE. Output saved to: {OUTPUT_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n!pip install \"numpy<2.0\" \"opencv-python-headless<4.10\" \"ultralytics\" \"onnxruntime-gpu\"\n\nimport cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nimport os\n\n# --- 1. CONFIGURATION ---\nYOLO_PATH = '/kaggle/input/yolo-v8/other/default/1/yolov8m.pt'\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx' \nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_1.mp4'\nOUTPUT_PATH = '/kaggle/working/output_snatching_ALARM.mp4'\n\nINPUT_H, INPUT_W = 256, 256\n\n# --- LOGIC SETTINGS ---\nSNATCH_TIME_THRESHOLD = 0.5  # Seconds\n# Hand Keypoints (Wrists). In COCO these are 9 (Left) and 10 (Right).\n# WARNING: In your Animal model, these might be \"Paws\". Look for Blue Dots!\nKEYPOINT_L_WRIST = 9\nKEYPOINT_R_WRIST = 10\n\n# --- 2. LOAD MODELS ---\nprint(\"Loading YOLOv8...\")\nyolo_model = YOLO(YOLO_PATH)\n\nprint(\"Loading RTMPose...\")\ntry:\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=providers)\nexcept Exception as e:\n    print(f\"CUDA Error ({e}). Using CPU.\")\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\n\ninput_name = ort_session.get_inputs()[0].name\n\n# --- 3. HELPER FUNCTIONS ---\n\ndef get_neck_from_box(x1, y1, x2, y2):\n    \"\"\"Calculates Geometric Neck Zone from Bounding Box.\"\"\"\n    box_h = y2 - y1\n    box_w = x2 - x1\n    neck_x = x1 + (box_w / 2)\n    neck_y = y1 + (box_h * 0.20) \n    radius = int(box_h * 0.15)\n    return (int(neck_x), int(neck_y)), radius\n\ndef is_point_in_circle(point, circle_center, radius):\n    \"\"\"Returns True if point (x,y) is inside the circle.\"\"\"\n    dist = np.sqrt((point[0] - circle_center[0])**2 + (point[1] - circle_center[1])**2)\n    return dist < radius\n\ndef preprocess_for_rtmpose(img_crop, h, w):\n    resized = cv2.resize(img_crop, (w, h))\n    mean = np.array([123.675, 116.28, 103.53])\n    std = np.array([58.395, 57.12, 57.375])\n    img_data = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_data = (img_data - mean) / std\n    img_data = img_data.astype(np.float32).transpose(2, 0, 1)\n    img_data = np.expand_dims(img_data, axis=0)\n    return img_data\n\ndef decode_simcc_output(outputs, crop_h, crop_w):\n    simcc_x, simcc_y = outputs[0], outputs[1]\n    x_locs = np.argmax(simcc_x, axis=2)[0]\n    y_locs = np.argmax(simcc_y, axis=2)[0]\n    num_bins_x = simcc_x.shape[2]\n    num_bins_y = simcc_y.shape[2]\n    x_locs = x_locs / (num_bins_x) * crop_w\n    y_locs = y_locs / (num_bins_y) * crop_h\n    return np.stack([x_locs, y_locs], axis=-1)\n\n# --- 4. PROCESSING LOOP ---\nif not os.path.exists(VIDEO_PATH):\n    print(\"Video not found.\")\n    exit()\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps != fps or fps <= 0: fps = 25\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n# Calculate required frames for alarm\nREQUIRED_FRAMES = int(SNATCH_TIME_THRESHOLD * fps)\nprint(f\"Logic: Trigger Alarm if hand in Neck Zone for {REQUIRED_FRAMES} frames ({SNATCH_TIME_THRESHOLD}s)\")\n\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(fps), (width, height))\n\n# --- THE MEMORY BANK ---\n# format: { track_id: consecutive_frames_in_zone }\nsnatch_history = {}\n\nframe_idx = 0\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    if frame is None or frame.size == 0: continue\n\n    frame_idx += 1\n    if frame_idx % 20 == 0:\n        print(f\"Processing frame {frame_idx}/{total_frames}...\")\n\n    # 1. TRACKING (model.track instead of predict)\n    # persist=True keeps IDs across frames\n    results = yolo_model.track(frame, classes=0, persist=True, verbose=False, conf=0.5, tracker=\"bytetrack.yaml\")\n\n    for result in results:\n        boxes = result.boxes\n        \n        # If no one detected, skip\n        if boxes.id is None:\n            continue\n            \n        # Get IDs and Coordinates\n        track_ids = boxes.id.int().cpu().tolist()\n        coords = boxes.xyxy.int().cpu().tolist()\n        \n        for box_id, (x1, y1, x2, y2) in zip(track_ids, coords):\n            # Ensure ID is in history\n            if box_id not in snatch_history:\n                snatch_history[box_id] = 0\n            \n            # --- A. DRAW NECK ZONE ---\n            neck_center, neck_radius = get_neck_from_box(x1, y1, x2, y2)\n            \n            # --- B. GET POSE ---\n            hand_in_zone = False\n            \n            x1_c, y1_c = max(0, x1), max(0, y1)\n            x2_c, y2_c = min(width, x2), min(height, y2)\n            person_crop = frame[y1_c:y2_c, x1_c:x2_c]\n            \n            if person_crop.size > 0:\n                crop_h, crop_w = person_crop.shape[:2]\n                onnx_input = preprocess_for_rtmpose(person_crop, INPUT_H, INPUT_W)\n                outputs = ort_session.run(None, {input_name: onnx_input})\n                \n                if len(outputs) >= 2:\n                    kpts_crop = decode_simcc_output(outputs, crop_h, crop_w)\n                    \n                    # Check Left Hand (9) and Right Hand (10)\n                    for k_idx in [KEYPOINT_L_WRIST, KEYPOINT_R_WRIST]:\n                        if k_idx < len(kpts_crop):\n                            kx, ky = kpts_crop[k_idx]\n                            real_x, real_y = int(kx + x1_c), int(ky + y1_c)\n                            \n                            # DEBUG: Draw Hands as BLUE DOTS\n                            cv2.circle(frame, (real_x, real_y), 4, (255, 0, 0), -1)\n                            \n                            # CHECK PROXIMITY\n                            if is_point_in_circle((real_x, real_y), neck_center, neck_radius):\n                                hand_in_zone = True\n\n            # --- C. UPDATE HISTORY LOGIC ---\n            if hand_in_zone:\n                snatch_history[box_id] += 1\n            else:\n                # Reset if hand leaves zone (optional: or decay slowly)\n                snatch_history[box_id] = 0\n            \n            # --- D. VISUALIZE STATUS ---\n            current_duration = snatch_history[box_id]\n            \n            # DEFAULT: GREEN (Safe)\n            color = (0, 255, 0)\n            status_text = f\"ID:{box_id}\"\n            \n            # WARNING: ORANGE (Hand in zone but not long enough)\n            if current_duration > 0 and current_duration < REQUIRED_FRAMES:\n                color = (0, 165, 255) # Orange-ish\n                status_text = f\"Warn: {current_duration}/{REQUIRED_FRAMES}\"\n                cv2.circle(frame, neck_center, neck_radius, color, 3)\n\n            # ALARM: RED (Snatch Detected!)\n            elif current_duration >= REQUIRED_FRAMES:\n                color = (0, 0, 255) # RED\n                status_text = \"SNATCH DETECTED!\"\n                \n                # Draw Big Alarm Box\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 4)\n                cv2.circle(frame, neck_center, neck_radius, color, -1) # Fill zone\n                \n                # Add Alarm Label\n                cv2.putText(frame, \"!!! SNATCH !!!\", (x1, y1 - 30), \n                           cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)\n\n            # Draw normal box if not alarming\n            if current_duration < REQUIRED_FRAMES:\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n                cv2.circle(frame, neck_center, neck_radius, (255, 255, 0), 2)\n            \n            cv2.putText(frame, status_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\nprint(f\"DONE. Logic Applied. Video saved to: {OUTPUT_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Uninstall the conflicting libraries\n!pip uninstall -y numpy ultralytics opencv-python opencv-python-headless onnxruntime onnxruntime-gpu\n\n# 2. Reinstall them with strict version matching\n# We force numpy<2.0 and older opencv to satisfy the environment's pre-compiled PyTorch\n!pip install \"numpy<2.0\" \"ultralytics==8.2.0\" \"opencv-python-headless<4.10\" \"onnxruntime-gpu\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nimport os\n\n# --- 1. CONFIGURATION ---\nYOLO_PATH = '/kaggle/input/yolo-v8/other/default/1/yolov8m.pt'\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx' \nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_1.mp4'\nOUTPUT_PATH = '/kaggle/working/output_snatching_ALARM.mp4'\n\nINPUT_H, INPUT_W = 256, 256\nSNATCH_TIME_THRESHOLD = 0.5  # Seconds\nREQUIRED_FRAMES = 15 # Will be recalculated based on FPS\n\n# --- 2. LOAD MODELS ---\nprint(\"Loading YOLOv8...\")\nyolo_model = YOLO(YOLO_PATH)\n\nprint(\"Loading RTMPose...\")\ntry:\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=providers)\n    print(f\"RTMPose loaded on: {ort_session.get_providers()[0]}\")\nexcept Exception as e:\n    print(f\"CUDA Error ({e}). Using CPU.\")\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\n\ninput_name = ort_session.get_inputs()[0].name\n\n# --- 3. HELPER FUNCTIONS ---\ndef get_neck_from_box(x1, y1, x2, y2):\n    \"\"\"Calculates Geometric Neck Zone from Bounding Box.\"\"\"\n    box_h = y2 - y1\n    box_w = x2 - x1\n    neck_x = x1 + (box_w / 2)\n    neck_y = y1 + (box_h * 0.20) \n    radius = int(box_h * 0.15)\n    return (int(neck_x), int(neck_y)), radius\n\ndef is_point_in_circle(point, circle_center, radius):\n    \"\"\"True if point is inside circle.\"\"\"\n    dist = np.sqrt((point[0] - circle_center[0])**2 + (point[1] - circle_center[1])**2)\n    return dist < radius\n\ndef preprocess_for_rtmpose(img_crop, h, w):\n    resized = cv2.resize(img_crop, (w, h))\n    mean = np.array([123.675, 116.28, 103.53])\n    std = np.array([58.395, 57.12, 57.375])\n    img_data = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_data = (img_data - mean) / std\n    img_data = img_data.astype(np.float32).transpose(2, 0, 1)\n    img_data = np.expand_dims(img_data, axis=0)\n    return img_data\n\ndef decode_simcc_output(outputs, crop_h, crop_w):\n    simcc_x, simcc_y = outputs[0], outputs[1]\n    x_locs = np.argmax(simcc_x, axis=2)[0]\n    y_locs = np.argmax(simcc_y, axis=2)[0]\n    num_bins_x = simcc_x.shape[2]\n    num_bins_y = simcc_y.shape[2]\n    x_locs = x_locs / (num_bins_x) * crop_w\n    y_locs = y_locs / (num_bins_y) * crop_h\n    return np.stack([x_locs, y_locs], axis=-1)\n\n# --- 4. PROCESSING LOOP ---\nif not os.path.exists(VIDEO_PATH):\n    print(\"Video not found.\")\n    exit()\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps != fps or fps <= 0: fps = 25\nREQUIRED_FRAMES = int(SNATCH_TIME_THRESHOLD * fps)\n\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(fps), (width, height))\nsnatch_history = {} # { track_id: consecutive_frames }\n\nprint(f\"Processing... Alarm triggers at {REQUIRED_FRAMES} frames.\")\nframe_idx = 0\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    if frame is None or frame.size == 0: continue\n\n    frame_idx += 1\n    if frame_idx % 20 == 0: print(f\"Processing frame {frame_idx}\")\n\n    # Track with persist=True\n    results = yolo_model.track(frame, classes=0, persist=True, verbose=False, conf=0.5, tracker=\"bytetrack.yaml\")\n\n    for result in results:\n        boxes = result.boxes\n        if boxes.id is None: continue\n        \n        track_ids = boxes.id.int().cpu().tolist()\n        coords = boxes.xyxy.int().cpu().tolist()\n        \n        for box_id, (x1, y1, x2, y2) in zip(track_ids, coords):\n            if box_id not in snatch_history: snatch_history[box_id] = 0\n            \n            # 1. Neck Zone\n            neck_center, neck_radius = get_neck_from_box(x1, y1, x2, y2)\n            \n            # 2. Check Hands\n            hand_in_zone = False\n            x1_c, y1_c = max(0, x1), max(0, y1)\n            x2_c, y2_c = min(width, x2), min(height, y2)\n            person_crop = frame[y1_c:y2_c, x1_c:x2_c]\n            \n            if person_crop.size > 0:\n                onnx_input = preprocess_for_rtmpose(person_crop, INPUT_H, INPUT_W)\n                outputs = ort_session.run(None, {input_name: onnx_input})\n                kpts = decode_simcc_output(outputs, person_crop.shape[0], person_crop.shape[1])\n                \n                # Check wrists (Indices 9 & 10)\n                for k in [9, 10]: \n                    if k < len(kpts):\n                        kx, ky = kpts[k]\n                        real_x, real_y = int(kx + x1_c), int(ky + y1_c)\n                        cv2.circle(frame, (real_x, real_y), 3, (255, 0, 0), -1) # Blue Dot\n                        if is_point_in_circle((real_x, real_y), neck_center, neck_radius):\n                            hand_in_zone = True\n\n            # 3. Update Status\n            if hand_in_zone: snatch_history[box_id] += 1\n            else: snatch_history[box_id] = max(0, snatch_history[box_id] - 1) # Decay\n            \n            # 4. Draw\n            cnt = snatch_history[box_id]\n            color = (0, 255, 0)\n            if cnt > 0: color = (0, 165, 255) # Orange\n            if cnt >= REQUIRED_FRAMES: \n                color = (0, 0, 255) # Red\n                cv2.putText(frame, \"SNATCH!\", (x1, y1-30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3)\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 4)\n            else:\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n            \n            cv2.circle(frame, neck_center, neck_radius, (255, 255, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\nprint(f\"DONE! Saved to {OUTPUT_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nimport os\n\n# --- 1. THE \"MAGIC FIX\" FOR PYTORCH 2.6 ---\n# This overrides the new strict security check so YOLO can load\n_original_load = torch.load\ndef safe_load_wrapper(*args, **kwargs):\n    # Force weights_only=False to allow loading older YOLO models\n    if 'weights_only' not in kwargs:\n        kwargs['weights_only'] = False\n    return _original_load(*args, **kwargs)\ntorch.load = safe_load_wrapper\n# ------------------------------------------\n\n# --- 2. CONFIGURATION ---\nYOLO_PATH = '/kaggle/input/yolo-v8/other/default/1/yolov8m.pt'\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx' \nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_1.mp4'\nOUTPUT_PATH = '/kaggle/working/output_snatching_ALARM.mp4'\n\nINPUT_H, INPUT_W = 256, 256\nSNATCH_TIME_THRESHOLD = 0.5  # Seconds for alarm\n\n# --- 3. LOAD MODELS ---\nprint(\"Loading YOLOv8...\")\nyolo_model = YOLO(YOLO_PATH)\n\nprint(\"Loading RTMPose...\")\ntry:\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=providers)\n    print(f\"RTMPose loaded on: {ort_session.get_providers()[0]}\")\nexcept Exception as e:\n    print(f\"CUDA Error ({e}). Using CPU.\")\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\n\ninput_name = ort_session.get_inputs()[0].name\n\n# --- 4. HELPER FUNCTIONS ---\n\ndef get_neck_from_box(x1, y1, x2, y2):\n    \"\"\"\n    Calculates Geometric Neck Zone from Bounding Box.\n    Neck is roughly at top 20% of the bounding box.\n    \"\"\"\n    box_h = y2 - y1\n    box_w = x2 - x1\n    neck_x = x1 + (box_w / 2)\n    neck_y = y1 + (box_h * 0.20) \n    radius = int(box_h * 0.15)\n    return (int(neck_x), int(neck_y)), radius\n\ndef is_point_in_circle(point, circle_center, radius):\n    dist = np.sqrt((point[0] - circle_center[0])**2 + (point[1] - circle_center[1])**2)\n    return dist < radius\n\ndef preprocess_for_rtmpose(img_crop, h, w):\n    resized = cv2.resize(img_crop, (w, h))\n    mean = np.array([123.675, 116.28, 103.53])\n    std = np.array([58.395, 57.12, 57.375])\n    img_data = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_data = (img_data - mean) / std\n    img_data = img_data.astype(np.float32).transpose(2, 0, 1)\n    img_data = np.expand_dims(img_data, axis=0)\n    return img_data\n\ndef decode_simcc_output(outputs, crop_h, crop_w):\n    simcc_x, simcc_y = outputs[0], outputs[1]\n    x_locs = np.argmax(simcc_x, axis=2)[0]\n    y_locs = np.argmax(simcc_y, axis=2)[0]\n    num_bins_x = simcc_x.shape[2]\n    num_bins_y = simcc_y.shape[2]\n    x_locs = x_locs / (num_bins_x) * crop_w\n    y_locs = y_locs / (num_bins_y) * crop_h\n    return np.stack([x_locs, y_locs], axis=-1)\n\n# --- 5. PROCESSING LOOP ---\nif not os.path.exists(VIDEO_PATH):\n    print(f\"Video not found at {VIDEO_PATH}\")\n    exit()\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps != fps or fps <= 0: fps = 25\n\nREQUIRED_FRAMES = int(SNATCH_TIME_THRESHOLD * fps)\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(fps), (width, height))\n\n# Memory Bank: { track_id: consecutive_frames_in_zone }\nsnatch_history = {} \n\nprint(f\"Processing Video... Alarm triggers after {REQUIRED_FRAMES} frames.\")\n\nframe_idx = 0\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    if frame is None or frame.size == 0: continue\n\n    frame_idx += 1\n    if frame_idx % 20 == 0: \n        print(f\"Processing frame {frame_idx}/{total_frames}\")\n\n    # Track\n    results = yolo_model.track(frame, classes=0, persist=True, verbose=False, conf=0.5, tracker=\"bytetrack.yaml\")\n\n    for result in results:\n        boxes = result.boxes\n        if boxes.id is None: continue\n        \n        track_ids = boxes.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nimport os\n\n# --- 1. THE \"MAGIC FIX\" FOR PYTORCH 2.6 ---\n# This overrides the new strict security check so YOLO can load\n_original_load = torch.load\ndef safe_load_wrapper(*args, **kwargs):\n    # Force weights_only=False to allow loading older YOLO models\n    if 'weights_only' not in kwargs:\n        kwargs['weights_only'] = False\n    return _original_load(*args, **kwargs)\ntorch.load = safe_load_wrapper\n# ------------------------------------------\n\n# --- 2. CONFIGURATION ---\nYOLO_PATH = '/kaggle/input/yolo-v8/other/default/1/yolov8m.pt'\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx' \nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_1.mp4'\nOUTPUT_PATH = '/kaggle/working/output_snatching_ALARM.mp4'\n\nINPUT_H, INPUT_W = 256, 192\nSNATCH_TIME_THRESHOLD = 0.5  # Seconds for alarm\n\n# --- 3. LOAD MODELS ---\nprint(\"Loading YOLOv8...\")\nyolo_model = YOLO(YOLO_PATH)\n\nprint(\"Loading RTMPose...\")\ntry:\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=providers)\n    print(f\"RTMPose loaded on: {ort_session.get_providers()[0]}\")\nexcept Exception as e:\n    print(f\"CUDA Error ({e}). Using CPU.\")\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\n\ninput_name = ort_session.get_inputs()[0].name\n\n# --- 4. HELPER FUNCTIONS ---\n\ndef get_neck_from_box(x1, y1, x2, y2):\n    \"\"\"\n    Calculates Geometric Neck Zone from Bounding Box.\n    Neck is roughly at top 20% of the bounding box.\n    \"\"\"\n    box_h = y2 - y1\n    box_w = x2 - x1\n    neck_x = x1 + (box_w / 2)\n    neck_y = y1 + (box_h * 0.20) \n    radius = int(box_h * 0.15)\n    return (int(neck_x), int(neck_y)), radius\n\ndef is_point_in_circle(point, circle_center, radius):\n    dist = np.sqrt((point[0] - circle_center[0])**2 + (point[1] - circle_center[1])**2)\n    return dist < radius\n\ndef preprocess_for_rtmpose(img_crop, h, w):\n    resized = cv2.resize(img_crop, (w, h))\n    mean = np.array([123.675, 116.28, 103.53])\n    std = np.array([58.395, 57.12, 57.375])\n    img_data = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_data = (img_data - mean) / std\n    img_data = img_data.astype(np.float32).transpose(2, 0, 1)\n    img_data = np.expand_dims(img_data, axis=0)\n    return img_data\n\ndef decode_simcc_output(outputs, crop_h, crop_w):\n    simcc_x, simcc_y = outputs[0], outputs[1]\n    x_locs = np.argmax(simcc_x, axis=2)[0]\n    y_locs = np.argmax(simcc_y, axis=2)[0]\n    num_bins_x = simcc_x.shape[2]\n    num_bins_y = simcc_y.shape[2]\n    x_locs = x_locs / (num_bins_x) * crop_w\n    y_locs = y_locs / (num_bins_y) * crop_h\n    return np.stack([x_locs, y_locs], axis=-1)\n\n# --- 5. PROCESSING LOOP ---\nif not os.path.exists(VIDEO_PATH):\n    print(f\"Video not found at {VIDEO_PATH}\")\n    exit()\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps != fps or fps <= 0: fps = 25\n\nREQUIRED_FRAMES = int(SNATCH_TIME_THRESHOLD * fps)\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(fps), (width, height))\n\n# Memory Bank: { track_id: consecutive_frames_in_zone }\nsnatch_history = {} \n\nprint(f\"Processing Video... Alarm triggers after {REQUIRED_FRAMES} frames.\")\n\nframe_idx = 0\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    if frame is None or frame.size == 0: continue\n\n    frame_idx += 1\n    if frame_idx % 20 == 0: \n        print(f\"Processing frame {frame_idx}/{total_frames}\")\n\n    # Track\n    results = yolo_model.track(frame, classes=0, persist=True, verbose=False, conf=0.5, tracker=\"bytetrack.yaml\")\n\n    for result in results:\n        boxes = result.boxes\n        if boxes.id is None: continue\n        \n        track_ids = boxes.id.int().cpu().tolist()\n        coords = boxes.xyxy.int().cpu().tolist()\n        \n        for box_id, (x1, y1, x2, y2) in zip(track_ids, coords):\n            if box_id not in snatch_history: \n                snatch_history[box_id] = 0\n            \n            # Neck Zone\n            neck_center, neck_radius = get_neck_from_box(x1, y1, x2, y2)\n            \n            # Check Hands\n            hand_in_zone = False\n            x1_c, y1_c = max(0, x1), max(0, y1)\n            x2_c, y2_c = min(width, x2), min(height, y2)\n            person_crop = frame[y1_c:y2_c, x1_c:x2_c]\n            \n            if person_crop.size > 0:\n                onnx_input = preprocess_for_rtmpose(person_crop, INPUT_H, INPUT_W)\n                outputs = ort_session.run(None, {input_name: onnx_input})\n                kpts = decode_simcc_output(outputs, person_crop.shape[0], person_crop.shape[1])\n                \n                # Check wrists (Indices 9 & 10)\n                for k in [9, 10]: \n                    if k < len(kpts):\n                        kx, ky = kpts[k]\n                        real_x, real_y = int(kx + x1_c), int(ky + y1_c)\n                        cv2.circle(frame, (real_x, real_y), 3, (255, 0, 0), -1) \n                        if is_point_in_circle((real_x, real_y), neck_center, neck_radius):\n                            hand_in_zone = True\n\n            # Logic\n            if hand_in_zone: \n                snatch_history[box_id] += 1\n            else: \n                snatch_history[box_id] = max(0, snatch_history[box_id] - 1) \n            \n            # Draw\n            cnt = snatch_history[box_id]\n            color = (0, 255, 0)\n            status_text = f\"ID:{box_id}\"\n            \n            if cnt > 0: \n                color = (0, 165, 255) # Orange\n                status_text = f\"Check: {cnt}\"\n            \n            if cnt >= REQUIRED_FRAMES: \n                color = (0, 0, 255) # Red\n                cv2.putText(frame, \"SNATCH!\", (x1, y1-30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3)\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 4)\n                cv2.circle(frame, neck_center, neck_radius, color, -1)\n            else:\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n                cv2.circle(frame, neck_center, neck_radius, (255, 255, 0), 2)\n            \n            cv2.putText(frame, status_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\nprint(f\"DONE! Video saved to {OUTPUT_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nimport os\n\n# --- 1. THE \"MAGIC FIX\" FOR PYTORCH 2.6 ---\n# This overrides the new strict security check so YOLO can load\n_original_load = torch.load\ndef safe_load_wrapper(*args, **kwargs):\n    # Force weights_only=False to allow loading older YOLO models\n    if 'weights_only' not in kwargs:\n        kwargs['weights_only'] = False\n    return _original_load(*args, **kwargs)\ntorch.load = safe_load_wrapper\n# ------------------------------------------\n\n# --- 2. CONFIGURATION ---\nYOLO_PATH = '/kaggle/input/yolo-v8/other/default/1/yolov8m.pt'\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx' \nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_1.mp4'\nOUTPUT_PATH = '/kaggle/working/output_snatching_ALARM.mp4'\n\nINPUT_H, INPUT_W = 256, 192\nSNATCH_TIME_THRESHOLD = 0.5  # Seconds for alarm\n\n# --- 3. LOAD MODELS ---\nprint(\"Loading YOLOv8...\")\nyolo_model = YOLO(YOLO_PATH)\n\nprint(\"Loading RTMPose...\")\ntry:\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=providers)\n    print(f\"RTMPose loaded on: {ort_session.get_providers()[0]}\")\nexcept Exception as e:\n    print(f\"CUDA Error ({e}). Using CPU.\")\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\n\ninput_name = ort_session.get_inputs()[0].name\n\n# --- 4. HELPER FUNCTIONS ---\n\ndef get_neck_from_box(x1, y1, x2, y2):\n    \"\"\"\n    Calculates Geometric Neck Zone from Bounding Box.\n    Neck is roughly at top 20% of the bounding box.\n    \"\"\"\n    box_h = y2 - y1\n    box_w = x2 - x1\n    neck_x = x1 + (box_w / 2)\n    neck_y = y1 + (box_h * 0.20) \n    radius = int(box_h * 0.15)\n    return (int(neck_x), int(neck_y)), radius\n\ndef is_point_in_circle(point, circle_center, radius):\n    dist = np.sqrt((point[0] - circle_center[0])**2 + (point[1] - circle_center[1])**2)\n    return dist < radius\n\ndef preprocess_for_rtmpose(img_crop, h, w):\n    resized = cv2.resize(img_crop, (w, h))\n    mean = np.array([123.675, 116.28, 103.53])\n    std = np.array([58.395, 57.12, 57.375])\n    img_data = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_data = (img_data - mean) / std\n    img_data = img_data.astype(np.float32).transpose(2, 0, 1)\n    img_data = np.expand_dims(img_data, axis=0)\n    return img_data\n\ndef decode_simcc_output(outputs, crop_h, crop_w):\n    simcc_x, simcc_y = outputs[0], outputs[1]\n    x_locs = np.argmax(simcc_x, axis=2)[0]\n    y_locs = np.argmax(simcc_y, axis=2)[0]\n    num_bins_x = simcc_x.shape[2]\n    num_bins_y = simcc_y.shape[2]\n    x_locs = x_locs / (num_bins_x) * crop_w\n    y_locs = y_locs / (num_bins_y) * crop_h\n    return np.stack([x_locs, y_locs], axis=-1)\n\n# --- 5. PROCESSING LOOP ---\nif not os.path.exists(VIDEO_PATH):\n    print(f\"Video not found at {VIDEO_PATH}\")\n    exit()\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps != fps or fps <= 0: fps = 25\n\nREQUIRED_FRAMES = int(SNATCH_TIME_THRESHOLD * fps)\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(fps), (width, height))\n\n# Memory Bank: { track_id: consecutive_frames_in_zone }\nsnatch_history = {} \n\nprint(f\"Processing Video... Alarm triggers after {REQUIRED_FRAMES} frames.\")\n\nframe_idx = 0\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    if frame is None or frame.size == 0: continue\n\n    frame_idx += 1\n    if frame_idx % 20 == 0: \n        print(f\"Processing frame {frame_idx}/{total_frames}\")\n\n    # Track\n    results = yolo_model.track(frame, classes=0, persist=True, verbose=False, conf=0.5, tracker=\"bytetrack.yaml\")\n\n    for result in results:\n        boxes = result.boxes\n        if boxes.id is None: continue\n        \n        track_ids = boxes.id.int().cpu().tolist()\n        coords = boxes.xyxy.int().cpu().tolist()\n        \n        for box_id, (x1, y1, x2, y2) in zip(track_ids, coords):\n            if box_id not in snatch_history: \n                snatch_history[box_id] = 0\n            \n            # Neck Zone\n            neck_center, neck_radius = get_neck_from_box(x1, y1, x2, y2)\n            \n            # Check Hands\n            hand_in_zone = False\n            x1_c, y1_c = max(0, x1), max(0, y1)\n            x2_c, y2_c = min(width, x2), min(height, y2)\n            person_crop = frame[y1_c:y2_c, x1_c:x2_c]\n            \n            if person_crop.size > 0:\n                onnx_input = preprocess_for_rtmpose(person_crop, INPUT_H, INPUT_W)\n                outputs = ort_session.run(None, {input_name: onnx_input})\n                kpts = decode_simcc_output(outputs, person_crop.shape[0], person_crop.shape[1])\n                \n                # Check wrists (Indices 9 & 10)\n                for k in [9, 10]: \n                    if k < len(kpts):\n                        kx, ky = kpts[k]\n                        real_x, real_y = int(kx + x1_c), int(ky + y1_c)\n                        cv2.circle(frame, (real_x, real_y), 3, (255, 0, 0), -1) \n                        if is_point_in_circle((real_x, real_y), neck_center, neck_radius):\n                            hand_in_zone = True\n\n            # Logic\n            if hand_in_zone: \n                snatch_history[box_id] += 1\n            else: \n                snatch_history[box_id] = max(0, snatch_history[box_id] - 1) \n            \n            # Draw\n            cnt = snatch_history[box_id]\n            color = (0, 255, 0)\n            status_text = f\"ID:{box_id}\"\n            \n            if cnt > 0: \n                color = (0, 165, 255) # Orange\n                status_text = f\"Check: {cnt}\"\n            \n            if cnt >= REQUIRED_FRAMES: \n                color = (0, 0, 255) # Red\n                cv2.putText(frame, \"SNATCH!\", (x1, y1-30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3)\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 4)\n                cv2.circle(frame, neck_center, neck_radius, color, -1)\n            else:\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n                cv2.circle(frame, neck_center, neck_radius, (255, 255, 0), 2)\n            \n            cv2.putText(frame, status_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\nprint(f\"DONE! Video saved to {OUTPUT_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nimport os\n\n# --- 1. THE \"MAGIC FIX\" (Safety Version) ---\n# This version checks if we already fixed it, preventing the RecursionError\nif not hasattr(torch.load, \"_is_patched\"):\n    _original_load = torch.load\n    def safe_load_wrapper(*args, **kwargs):\n        if 'weights_only' not in kwargs:\n            kwargs['weights_only'] = False\n        return _original_load(*args, **kwargs)\n    safe_load_wrapper._is_patched = True\n    torch.load = safe_load_wrapper\n# -------------------------------------------\n\n# --- 2. CONFIGURATION ---\nYOLO_PATH = '/kaggle/input/yolo-v8/other/default/1/yolov8m.pt'\n\n# UPDATE THIS PATH to your new Human Pose Model (COCO)\n# Example: '/kaggle/input/rtmpose-coco/rtmpose-m_coco_256x192.onnx'\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx' \n\nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_1.mp4'\nOUTPUT_PATH = '/kaggle/working/output_snatching_ALARM_v2.mp4'\n\n# Update Input Size for Human Model (usually 192 width)\nINPUT_H, INPUT_W = 256, 192 \nSNATCH_TIME_THRESHOLD = 0.5\n\n# --- 3. LOAD MODELS ---\nprint(\"Loading YOLOv8...\")\nyolo_model = YOLO(YOLO_PATH)\n\nprint(\"Loading RTMPose...\")\ntry:\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=providers)\n    print(f\"RTMPose loaded on: {ort_session.get_providers()[0]}\")\nexcept Exception as e:\n    print(f\"CUDA Error ({e}). Using CPU.\")\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\n\ninput_name = ort_session.get_inputs()[0].name\n\n# --- 4. HELPER FUNCTIONS ---\n\ndef get_neck_from_box(x1, y1, x2, y2):\n    \"\"\"Calculates Geometric Neck Zone from Bounding Box (Top 20%).\"\"\"\n    box_h = y2 - y1\n    box_w = x2 - x1\n    neck_x = x1 + (box_w / 2)\n    neck_y = y1 + (box_h * 0.20) \n    radius = int(box_h * 0.15)\n    return (int(neck_x), int(neck_y)), radius\n\ndef is_point_in_circle(point, circle_center, radius):\n    \"\"\"True if point is inside circle.\"\"\"\n    dist = np.sqrt((point[0] - circle_center[0])**2 + (point[1] - circle_center[1])**2)\n    return dist < radius\n\ndef preprocess_for_rtmpose(img_crop, h, w):\n    resized = cv2.resize(img_crop, (w, h))\n    mean = np.array([123.675, 116.28, 103.53])\n    std = np.array([58.395, 57.12, 57.375])\n    img_data = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_data = (img_data - mean) / std\n    img_data = img_data.astype(np.float32).transpose(2, 0, 1)\n    img_data = np.expand_dims(img_data, axis=0)\n    return img_data\n\ndef decode_simcc_output(outputs, crop_h, crop_w):\n    simcc_x, simcc_y = outputs[0], outputs[1]\n    x_locs = np.argmax(simcc_x, axis=2)[0]\n    y_locs = np.argmax(simcc_y, axis=2)[0]\n    num_bins_x = simcc_x.shape[2]\n    num_bins_y = simcc_y.shape[2]\n    x_locs = x_locs / (num_bins_x) * crop_w\n    y_locs = y_locs / (num_bins_y) * crop_h\n    return np.stack([x_locs, y_locs], axis=-1)\n\n# --- 5. PROCESSING LOOP ---\nif not os.path.exists(VIDEO_PATH):\n    print(f\"Video not found at {VIDEO_PATH}\")\n    exit()\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Safety check for FPS\nif fps != fps or fps <= 0: \n    fps = 25\n\nREQUIRED_FRAMES = int(SNATCH_TIME_THRESHOLD * fps)\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(fps), (width, height))\n\n# Memory Bank: { track_id: consecutive_frames_in_zone }\nsnatch_history = {} \n\nprint(f\"Processing Video... Alarm triggers after {REQUIRED_FRAMES} frames.\")\n\nframe_idx = 0\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    if frame is None or frame.size == 0: continue\n\n    frame_idx += 1\n    if frame_idx % 20 == 0: \n        print(f\"Processing frame {frame_idx}/{total_frames}\")\n\n    # 1. TRACKING\n    results = yolo_model.track(frame, classes=0, persist=True, verbose=False, conf=0.5, tracker=\"bytetrack.yaml\")\n\n    for result in results:\n        boxes = result.boxes\n        if boxes.id is None: continue\n        \n        track_ids = boxes.id.int().cpu().tolist()\n        coords = boxes.xyxy.int().cpu().tolist()\n        \n        for box_id, (x1, y1, x2, y2) in zip(track_ids, coords):\n            if box_id not in snatch_history: \n                snatch_history[box_id] = 0\n            \n            # --- A. NECK ZONE ---\n            neck_center, neck_radius = get_neck_from_box(x1, y1, x2, y2)\n            \n            # --- B. CHECK HANDS ---\n            hand_in_zone = False\n            x1_c, y1_c = max(0, x1), max(0, y1)\n            x2_c, y2_c = min(width, x2), min(height, y2)\n            person_crop = frame[y1_c:y2_c, x1_c:x2_c]\n            \n            if person_crop.size > 0:\n                onnx_input = preprocess_for_rtmpose(person_crop, INPUT_H, INPUT_W)\n                outputs = ort_session.run(None, {input_name: onnx_input})\n                kpts = decode_simcc_output(outputs, person_crop.shape[0], person_crop.shape[1])\n                \n                # Check wrists (Indices 9 & 10 for COCO Humans)\n                for k in [9, 10]: \n                    if k < len(kpts):\n                        kx, ky = kpts[k]\n                        real_x, real_y = int(kx + x1_c), int(ky + y1_c)\n                        \n                        # Draw Blue Dot for Hand\n                        cv2.circle(frame, (real_x, real_y), 3, (255, 0, 0), -1) \n                        \n                        if is_point_in_circle((real_x, real_y), neck_center, neck_radius):\n                            hand_in_zone = True\n\n            # --- C. UPDATE LOGIC ---\n            if hand_in_zone: \n                snatch_history[box_id] += 1\n            else: \n                snatch_history[box_id] = max(0, snatch_history[box_id] - 1) \n            \n            # --- D. VISUALIZE ---\n            cnt = snatch_history[box_id]\n            \n            color = (0, 255, 0)\n            status_text = f\"ID:{box_id}\"\n            \n            if cnt > 0: \n                color = (0, 165, 255) # Orange\n                status_text = f\"Check: {cnt}\"\n            \n            if cnt >= REQUIRED_FRAMES: \n                color = (0, 0, 255) # Red\n                # ALARM VISUALS\n                cv2.putText(frame, \"SNATCH!\", (x1, y1-30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3)\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 4)\n                cv2.circle(frame, neck_center, neck_radius, color, -1)\n            else:\n                # Normal Visuals\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n                cv2.circle(frame, neck_center, neck_radius, (255, 255, 0), 2)\n            \n            cv2.putText(frame, status_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\nprint(f\"DONE! Video saved to {OUTPUT_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Uninstall the conflicting libraries\n!pip uninstall -y numpy ultralytics opencv-python opencv-python-headless onnxruntime onnxruntime-gpu\n\n# 2. Reinstall them with strict version matching\n# We force numpy<2.0 and older opencv to satisfy the environment's pre-compiled PyTorch\n!pip install \"numpy<2.0\" \"ultralytics==8.2.0\" \"opencv-python-headless<4.10\" \"onnxruntime-gpu\"\n\n\n\nimport torch\nimport cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nimport os\n\n# --- SAFETY FIX FOR PYTORCH 2.6 ---\nif not hasattr(torch.load, \"_is_patched\"):\n    _original_load = torch.load\n    def safe_load_wrapper(*args, **kwargs):\n        if 'weights_only' not in kwargs: kwargs['weights_only'] = False\n        return _original_load(*args, **kwargs)\n    safe_load_wrapper._is_patched = True\n    torch.load = safe_load_wrapper\n# ----------------------------------\n\n# CONFIGURATION\nYOLO_PATH = '/kaggle/input/yolo-v8/other/default/1/yolov8m.pt'\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx' \nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_11.mp4'\nOUTPUT_PATH = '/kaggle/working/output_debug_indices.mp4'\n\nINPUT_H, INPUT_W = 256, 256 # Keeping square for Animal model\n\n# LOAD MODELS\nyolo_model = YOLO(YOLO_PATH)\ntry:\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=providers)\nexcept:\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\ninput_name = ort_session.get_inputs()[0].name\n\n# HELPERS\ndef preprocess_for_rtmpose(img_crop, h, w):\n    resized = cv2.resize(img_crop, (w, h))\n    mean = np.array([123.675, 116.28, 103.53])\n    std = np.array([58.395, 57.12, 57.375])\n    img_data = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_data = (img_data - mean) / std\n    img_data = img_data.astype(np.float32).transpose(2, 0, 1)\n    img_data = np.expand_dims(img_data, axis=0)\n    return img_data\n\ndef decode_simcc_output(outputs, crop_h, crop_w):\n    simcc_x, simcc_y = outputs[0], outputs[1]\n    x_locs = np.argmax(simcc_x, axis=2)[0]\n    y_locs = np.argmax(simcc_y, axis=2)[0]\n    num_bins_x = simcc_x.shape[2]\n    num_bins_y = simcc_y.shape[2]\n    x_locs = x_locs / (num_bins_x) * crop_w\n    y_locs = y_locs / (num_bins_y) * crop_h\n    return np.stack([x_locs, y_locs], axis=-1)\n\n# PROCESSING LOOP\ncap = cv2.VideoCapture(VIDEO_PATH)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps != fps or fps <= 0: fps = 25\n\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(fps), (width, height))\n\nprint(\"Generating Diagnostic Video... Look for the number on the HANDS.\")\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    \n    # Track\n    results = yolo_model.track(frame, classes=0, persist=True, verbose=False, conf=0.5, tracker=\"bytetrack.yaml\")\n\n    for result in results:\n        boxes = result.boxes\n        if boxes.id is None: continue\n        \n        coords = boxes.xyxy.int().cpu().tolist()\n        \n        for (x1, y1, x2, y2) in coords:\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 1)\n            \n            x1_c, y1_c = max(0, x1), max(0, y1)\n            x2_c, y2_c = min(width, x2), min(height, y2)\n            person_crop = frame[y1_c:y2_c, x1_c:x2_c]\n            \n            if person_crop.size > 0:\n                onnx_input = preprocess_for_rtmpose(person_crop, INPUT_H, INPUT_W)\n                outputs = ort_session.run(None, {input_name: onnx_input})\n                kpts = decode_simcc_output(outputs, person_crop.shape[0], person_crop.shape[1])\n                \n                # DRAW EVERY NUMBER\n                for i, (kx, ky) in enumerate(kpts):\n                    real_x, real_y = int(kx + x1_c), int(ky + y1_c)\n                    \n                    # Draw White Dot\n                    cv2.circle(frame, (real_x, real_y), 3, (255, 255, 255), -1)\n                    # Draw Number in Blue\n                    cv2.putText(frame, str(i), (real_x+4, real_y), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 0), 1)\n\n    out.write(frame)\n\ncap.release()\nout.release()\nprint(f\"DONE. Watch {OUTPUT_PATH} and find the hand number.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fix the conflict between Numpy and Scipy\n!pip install \"numpy<2.0\" \"scipy<1.13\" --force-reinstall","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nimport os\n\n# --- 1. PYTORCH SAFETY PATCH (Prevents RecursionError) ---\nif not hasattr(torch.load, \"_is_patched\"):\n    _original_load = torch.load\n    def safe_load_wrapper(*args, **kwargs):\n        if 'weights_only' not in kwargs:\n            kwargs['weights_only'] = False\n        return _original_load(*args, **kwargs)\n    safe_load_wrapper._is_patched = True\n    torch.load = safe_load_wrapper\n# ---------------------------------------------------------\n\n# --- 2. CONFIGURATION ---\nYOLO_PATH = '/kaggle/input/yolo-v8/other/default/1/yolov8m.pt'\n#  Ensure this path points to your model (Animal or Human)\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx' \nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_1.mp4'\nOUTPUT_PATH = '/kaggle/working/output_snatching_FINAL.mp4'\n\n# Input Size: 256 for Animal, 192 for Human\nINPUT_H, INPUT_W = 256, 256 \nSNATCH_TIME_THRESHOLD = 0.5\n\n# --- 3. LOAD MODELS ---\nprint(\"Loading YOLOv8...\")\nyolo_model = YOLO(YOLO_PATH)\n\nprint(\"Loading RTMPose...\")\ntry:\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=providers)\nexcept Exception as e:\n    print(f\"CUDA Error ({e}). Using CPU.\")\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\n\ninput_name = ort_session.get_inputs()[0].name\n\n# --- 4. HELPER FUNCTIONS ---\n\ndef get_neck_from_box(x1, y1, x2, y2):\n    \"\"\"Calculates Geometric Neck Zone from Bounding Box (Top 20%).\"\"\"\n    box_h = y2 - y1\n    box_w = x2 - x1\n    neck_x = x1 + (box_w / 2)\n    neck_y = y1 + (box_h * 0.20) \n    radius = int(box_h * 0.15)\n    return (int(neck_x), int(neck_y)), radius\n\ndef is_point_in_circle(point, circle_center, radius):\n    dist = np.sqrt((point[0] - circle_center[0])**2 + (point[1] - circle_center[1])**2)\n    return dist < radius\n\ndef preprocess_for_rtmpose(img_crop, h, w):\n    resized = cv2.resize(img_crop, (w, h))\n    mean = np.array([123.675, 116.28, 103.53])\n    std = np.array([58.395, 57.12, 57.375])\n    img_data = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_data = (img_data - mean) / std\n    img_data = img_data.astype(np.float32).transpose(2, 0, 1)\n    img_data = np.expand_dims(img_data, axis=0)\n    return img_data\n\ndef decode_simcc_output(outputs, crop_h, crop_w):\n    simcc_x, simcc_y = outputs[0], outputs[1]\n    x_locs = np.argmax(simcc_x, axis=2)[0]\n    y_locs = np.argmax(simcc_y, axis=2)[0]\n    num_bins_x = simcc_x.shape[2]\n    num_bins_y = simcc_y.shape[2]\n    x_locs = x_locs / (num_bins_x) * crop_w\n    y_locs = y_locs / (num_bins_y) * crop_h\n    return np.stack([x_locs, y_locs], axis=-1)\n\n# --- 5. PROCESSING LOOP ---\nif not os.path.exists(VIDEO_PATH):\n    print(\"Video not found.\")\n    exit()\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps != fps or fps <= 0: fps = 25\n\nREQUIRED_FRAMES = int(SNATCH_TIME_THRESHOLD * fps)\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(fps), (width, height))\nsnatch_history = {} \n\nprint(f\"Processing... Alarm at {REQUIRED_FRAMES} frames.\")\n\nframe_idx = 0\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    \n    frame_idx += 1\n    if frame_idx % 20 == 0: print(f\"Processing frame {frame_idx}/{total_frames}\")\n\n    # Track\n    results = yolo_model.track(frame, classes=0, persist=True, verbose=False, conf=0.5, tracker=\"bytetrack.yaml\")\n\n    for result in results:\n        boxes = result.boxes\n        if boxes.id is None: continue\n        \n        track_ids = boxes.id.int().cpu().tolist()\n        coords = boxes.xyxy.int().cpu().tolist()\n        \n        for box_id, (x1, y1, x2, y2) in zip(track_ids, coords):\n            if box_id not in snatch_history: snatch_history[box_id] = 0\n            \n            # Neck Zone\n            neck_center, neck_radius = get_neck_from_box(x1, y1, x2, y2)\n            \n            # Check Hands\n            hand_in_zone = False\n            x1_c, y1_c = max(0, x1), max(0, y1)\n            x2_c, y2_c = min(width, x2), min(height, y2)\n            person_crop = frame[y1_c:y2_c, x1_c:x2_c]\n            \n            if person_crop.size > 0:\n                onnx_input = preprocess_for_rtmpose(person_crop, INPUT_H, INPUT_W)\n                outputs = ort_session.run(None, {input_name: onnx_input})\n                kpts = decode_simcc_output(outputs, person_crop.shape[0], person_crop.shape[1])\n                \n                # Check indices. \n                # If using ANIMAL model, try checking indices 5,6,7,8 (Paws)\n                # If using HUMAN model, check 9,10 (Wrists)\n                indices_to_check = [9, 10] \n                \n                for k in indices_to_check: \n                    if k < len(kpts):\n                        kx, ky = kpts[k]\n                        real_x, real_y = int(kx + x1_c), int(ky + y1_c)\n                        cv2.circle(frame, (real_x, real_y), 3, (255, 0, 0), -1) \n                        if is_point_in_circle((real_x, real_y), neck_center, neck_radius):\n                            hand_in_zone = True\n\n            # Logic\n            if hand_in_zone: snatch_history[box_id] += 1\n            else: snatch_history[box_id] = max(0, snatch_history[box_id] - 1) \n            \n            # Draw\n            cnt = snatch_history[box_id]\n            color = (0, 255, 0)\n            status = f\"ID:{box_id}\"\n            \n            if cnt > 0: \n                color = (0, 165, 255)\n                status = f\"Check:{cnt}\"\n            if cnt >= REQUIRED_FRAMES: \n                color = (0, 0, 255)\n                cv2.putText(frame, \"SNATCH!\", (x1, y1-30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3)\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 4)\n                cv2.circle(frame, neck_center, neck_radius, color, -1)\n            else:\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n                cv2.circle(frame, neck_center, neck_radius, (255, 255, 0), 2)\n            \n            cv2.putText(frame, status, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\nprint(f\"DONE! Saved to {OUTPUT_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Force install exact compatible versions\n!pip install \"numpy==1.26.4\" \"scipy==1.11.4\" \"ultralytics\" \"opencv-python-headless\" --force-reinstall","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport torch\nimport cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nimport os\n\n# --- 1. PYTORCH SAFETY PATCH ---\nif not hasattr(torch.load, \"_is_patched\"):\n    _original_load = torch.load\n    def safe_load_wrapper(*args, **kwargs):\n        if 'weights_only' not in kwargs:\n            kwargs['weights_only'] = False\n        return _original_load(*args, **kwargs)\n    safe_load_wrapper._is_patched = True\n    torch.load = safe_load_wrapper\n# -------------------------------\n\n# --- 2. CONFIGURATION ---\nYOLO_PATH = '/kaggle/input/yolo-v8/other/default/1/yolov8m.pt'\n# PATH TO YOUR MODEL (Update if you uploaded a Human one)\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx' \n\n# *** UPDATED VIDEO PATH ***\nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_11.mp4'\nOUTPUT_PATH = '/kaggle/working/output_snatching_11_FINAL.mp4'\n\n# Input Size: 256 for Animal, 192 for Human\nINPUT_H, INPUT_W = 256, 256 \nSNATCH_TIME_THRESHOLD = 0.5\n\n# --- 3. LOAD MODELS ---\nprint(\"Loading YOLOv8...\")\nyolo_model = YOLO(YOLO_PATH)\n\nprint(\"Loading RTMPose...\")\ntry:\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=providers)\nexcept Exception as e:\n    print(f\"CUDA Error ({e}). Using CPU.\")\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\n\ninput_name = ort_session.get_inputs()[0].name\n\n# --- 4. HELPER FUNCTIONS ---\n\ndef get_neck_from_box(x1, y1, x2, y2):\n    \"\"\"Calculates Geometric Neck Zone from Bounding Box (Top 20%).\"\"\"\n    box_h = y2 - y1\n    box_w = x2 - x1\n    neck_x = x1 + (box_w / 2)\n    neck_y = y1 + (box_h * 0.20) \n    radius = int(box_h * 0.15)\n    return (int(neck_x), int(neck_y)), radius\n\ndef is_point_in_circle(point, circle_center, radius):\n    dist = np.sqrt((point[0] - circle_center[0])**2 + (point[1] - circle_center[1])**2)\n    return dist < radius\n\ndef preprocess_for_rtmpose(img_crop, h, w):\n    resized = cv2.resize(img_crop, (w, h))\n    mean = np.array([123.675, 116.28, 103.53])\n    std = np.array([58.395, 57.12, 57.375])\n    img_data = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_data = (img_data - mean) / std\n    img_data = img_data.astype(np.float32).transpose(2, 0, 1)\n    img_data = np.expand_dims(img_data, axis=0)\n    return img_data\n\ndef decode_simcc_output(outputs, crop_h, crop_w):\n    simcc_x, simcc_y = outputs[0], outputs[1]\n    x_locs = np.argmax(simcc_x, axis=2)[0]\n    y_locs = np.argmax(simcc_y, axis=2)[0]\n    num_bins_x = simcc_x.shape[2]\n    num_bins_y = simcc_y.shape[2]\n    x_locs = x_locs / (num_bins_x) * crop_w\n    y_locs = y_locs / (num_bins_y) * crop_h\n    return np.stack([x_locs, y_locs], axis=-1)\n\n# --- 5. PROCESSING LOOP ---\nif not os.path.exists(VIDEO_PATH):\n    print(f\"Error: Video not found at {VIDEO_PATH}\")\n    exit()\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps != fps or fps <= 0: fps = 25\n\nREQUIRED_FRAMES = int(SNATCH_TIME_THRESHOLD * fps)\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(fps), (width, height))\nsnatch_history = {} \n\nprint(f\"Processing... Alarm at {REQUIRED_FRAMES} frames.\")\n\nframe_idx = 0\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    \n    frame_idx += 1\n    if frame_idx % 20 == 0: print(f\"Processing frame {frame_idx}/{total_frames}\")\n\n    # Track\n    results = yolo_model.track(frame, classes=0, persist=True, verbose=False, conf=0.5, tracker=\"bytetrack.yaml\")\n\n    for result in results:\n        boxes = result.boxes\n        if boxes.id is None: continue\n        \n        track_ids = boxes.id.int().cpu().tolist()\n        coords = boxes.xyxy.int().cpu().tolist()\n        \n        for box_id, (x1, y1, x2, y2) in zip(track_ids, coords):\n            if box_id not in snatch_history: snatch_history[box_id] = 0\n            \n            # Neck Zone\n            neck_center, neck_radius = get_neck_from_box(x1, y1, x2, y2)\n            \n            # Check Hands\n            hand_in_zone = False\n            x1_c, y1_c = max(0, x1), max(0, y1)\n            x2_c, y2_c = min(width, x2), min(height, y2)\n            person_crop = frame[y1_c:y2_c, x1_c:x2_c]\n            \n            if person_crop.size > 0:\n                onnx_input = preprocess_for_rtmpose(person_crop, INPUT_H, INPUT_W)\n                outputs = ort_session.run(None, {input_name: onnx_input})\n                kpts = decode_simcc_output(outputs, person_crop.shape[0], person_crop.shape[1])\n                \n                # IMPORTANT: Indices to check for hands\n                # Animal Model: Try [9, 10] first. If wrong, try [5, 6].\n                indices_to_check = [9, 10] \n                \n                for k in indices_to_check: \n                    if k < len(kpts):\n                        kx, ky = kpts[k]\n                        real_x, real_y = int(kx + x1_c), int(ky + y1_c)\n                        # Draw Blue Dot\n                        cv2.circle(frame, (real_x, real_y), 3, (255, 0, 0), -1) \n                        if is_point_in_circle((real_x, real_y), neck_center, neck_radius):\n                            hand_in_zone = True\n\n            # Logic\n            if hand_in_zone: snatch_history[box_id] += 1\n            else: snatch_history[box_id] = max(0, snatch_history[box_id] - 1) \n            \n            # Draw\n            cnt = snatch_history[box_id]\n            color = (0, 255, 0)\n            status = f\"ID:{box_id}\"\n            \n            if cnt > 0: \n                color = (0, 165, 255)\n                status = f\"Check:{cnt}\"\n            if cnt >= REQUIRED_FRAMES: \n                color = (0, 0, 255)\n                cv2.putText(frame, \"SNATCH!\", (x1, y1-30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3)\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 4)\n                cv2.circle(frame, neck_center, neck_radius, color, -1)\n            else:\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n                cv2.circle(frame, neck_center, neck_radius, (255, 255, 0), 2)\n            \n            cv2.putText(frame, status, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\nprint(f\"DONE! Saved to {OUTPUT_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nimport os\n\nimport time  # Make sure this is imported\n\nclass SystemEvaluator:\n    def __init__(self):\n        # Confusion Matrix Counters (Rule #2: Metrics First)\n        self.true_positives = 0  \n        self.false_positives = 0 \n        self.false_negatives = 0 \n        self.true_negatives = 0  \n        \n        # Latency Tracking (Tuning Playbook: Input Pipeline Health)\n        self.total_time_ms = 0\n        self.frame_count = 0\n        self.start_t = 0\n\n    def start_timer(self):\n        self.start_t = time.time()\n\n    def stop_timer(self):\n        end_t = time.time()\n        duration_ms = (end_t - self.start_t) * 1000\n        self.total_time_ms += duration_ms\n        self.frame_count += 1\n        return duration_ms\n\n    def update_metrics(self, prediction, ground_truth):\n        if prediction == True and ground_truth == True:\n            self.true_positives += 1\n        elif prediction == True and ground_truth == False:\n            self.false_positives += 1  # False Alarm\n        elif prediction == False and ground_truth == True:\n            self.false_negatives += 1  # Missed Detection\n        elif prediction == False and ground_truth == False:\n            self.true_negatives += 1\n\n    def print_report(self):\n        avg_latency = self.total_time_ms / max(1, self.frame_count)\n        print(f\"\\n--- PERFORMANCE REPORT (Rules of ML Check) ---\")\n        print(f\"Avg Latency per Frame: {avg_latency:.2f} ms\")\n        print(f\"True Positives (Caught): {self.true_positives}\")\n        print(f\"False Positives (Alarms): {self.false_positives}\")\n        print(f\"False Negatives (Missed): {self.false_negatives}\")\n        print(f\"----------------------------------------------\")\n\n# --- 1. PYTORCH SAFETY PATCH ---\nif not hasattr(torch.load, \"_is_patched\"):\n    _original_load = torch.load\n    def safe_load_wrapper(*args, **kwargs):\n        if 'weights_only' not in kwargs:\n            kwargs['weights_only'] = False\n        return _original_load(*args, **kwargs)\n    safe_load_wrapper._is_patched = True\n    torch.load = safe_load_wrapper\n# -------------------------------\n\n# --- 2. CONFIGURATION ---\nYOLO_PATH = '/kaggle/input/yolo-v8/other/default/1/yolov8m.pt'\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx' \nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_11.mp4'\nOUTPUT_PATH = '/kaggle/working/output_snatching_11_IMPROVED.mp4'\n\nINPUT_H, INPUT_W = 256, 256 \n# LOWERED THRESHOLD: Catch faster snatches (0.3s instead of 0.5s)\nSNATCH_TIME_THRESHOLD = 0.3 \n\n# --- 3. LOAD MODELS ---\nprint(\"Loading YOLOv8...\")\nyolo_model = YOLO(YOLO_PATH)\n\nprint(\"Loading RTMPose...\")\ntry:\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=providers)\nexcept Exception as e:\n    print(f\"CUDA Error ({e}). Using CPU.\")\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\n\ninput_name = ort_session.get_inputs()[0].name\n\n# --- 4. HELPER FUNCTIONS ---\n\ndef get_neck_from_box(x1, y1, x2, y2):\n    \"\"\"Calculates Geometric Neck Zone from Bounding Box.\"\"\"\n    box_h = y2 - y1\n    box_w = x2 - x1\n    neck_x = x1 + (box_w / 2)\n    # Move neck slightly higher (15% from top instead of 20%) to catch bikers\n    neck_y = y1 + (box_h * 0.15) \n    radius = int(box_h * 0.15)\n    return (int(neck_x), int(neck_y)), radius\n\ndef is_point_in_circle(point, circle_center, radius):\n    dist = np.sqrt((point[0] - circle_center[0])**2 + (point[1] - circle_center[1])**2)\n    return dist < radius\n\ndef preprocess_for_rtmpose(img_crop, h, w):\n    resized = cv2.resize(img_crop, (w, h))\n    mean = np.array([123.675, 116.28, 103.53])\n    std = np.array([58.395, 57.12, 57.375])\n    img_data = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_data = (img_data - mean) / std\n    img_data = img_data.astype(np.float32).transpose(2, 0, 1)\n    img_data = np.expand_dims(img_data, axis=0)\n    return img_data\n\ndef decode_simcc_output(outputs, crop_h, crop_w):\n    simcc_x, simcc_y = outputs[0], outputs[1]\n    x_locs = np.argmax(simcc_x, axis=2)[0]\n    y_locs = np.argmax(simcc_y, axis=2)[0]\n    num_bins_x = simcc_x.shape[2]\n    num_bins_y = simcc_y.shape[2]\n    x_locs = x_locs / (num_bins_x) * crop_w\n    y_locs = y_locs / (num_bins_y) * crop_h\n    return np.stack([x_locs, y_locs], axis=-1)\n\n# --- 5. PROCESSING LOOP ---\nif not os.path.exists(VIDEO_PATH):\n    print(f\"Error: Video not found at {VIDEO_PATH}\")\n    exit()\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps != fps or fps <= 0: fps = 25\n\nREQUIRED_FRAMES = int(SNATCH_TIME_THRESHOLD * fps)\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(fps), (width, height))\nsnatch_history = {} \n\nprint(f\"Processing... Alarm triggers after {REQUIRED_FRAMES} frames.\")\n\nframe_idx = 0\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    \n    frame_idx += 1\n    if frame_idx % 20 == 0: print(f\"Processing frame {frame_idx}/{total_frames}\")\n\n    # Track\n    results = yolo_model.track(frame, classes=0, persist=True, verbose=False, conf=0.5, tracker=\"bytetrack.yaml\")\n\n    for result in results:\n        boxes = result.boxes\n        if boxes.id is None: continue\n        \n        track_ids = boxes.id.int().cpu().tolist()\n        coords = boxes.xyxy.int().cpu().tolist()\n        \n        for box_id, (x1, y1, x2, y2) in zip(track_ids, coords):\n            if box_id not in snatch_history: snatch_history[box_id] = 0\n            \n            # Neck Zone\n            neck_center, neck_radius = get_neck_from_box(x1, y1, x2, y2)\n            \n            # Check Hands\n            hand_in_zone = False\n            x1_c, y1_c = max(0, x1), max(0, y1)\n            x2_c, y2_c = min(width, x2), min(height, y2)\n            person_crop = frame[y1_c:y2_c, x1_c:x2_c]\n            \n            if person_crop.size > 0:\n                onnx_input = preprocess_for_rtmpose(person_crop, INPUT_H, INPUT_W)\n                outputs = ort_session.run(None, {input_name: onnx_input})\n                kpts = decode_simcc_output(outputs, person_crop.shape[0], person_crop.shape[1])\n                \n                # --- WIDE NET LOGIC ---\n                # Check ALL likely limb points (Front Paws 5-8, Back Paws 9-12)\n                # This increases the chance we catch the arm even if model is confused\n                indices_to_check = [5, 6, 7, 8, 9, 10, 11, 12] \n                \n                for k in indices_to_check: \n                    if k < len(kpts):\n                        kx, ky = kpts[k]\n                        real_x, real_y = int(kx + x1_c), int(ky + y1_c)\n                        \n                        # Only draw if near the person (filter out random noise points)\n                        if x1 <= real_x <= x2 and y1 <= real_y <= y2:\n                            # Draw Small Purple Dot for \"Limb Candidate\"\n                            cv2.circle(frame, (real_x, real_y), 2, (255, 0, 255), -1) \n                            \n                            if is_point_in_circle((real_x, real_y), neck_center, neck_radius):\n                                hand_in_zone = True\n\n            # Logic\n            if hand_in_zone: snatch_history[box_id] += 1\n            else: snatch_history[box_id] = max(0, snatch_history[box_id] - 1) \n            \n            # Draw\n            cnt = snatch_history[box_id]\n            color = (0, 255, 0)\n            status = f\"ID:{box_id}\"\n            \n            if cnt > 0: \n                color = (0, 165, 255) # Orange\n                status = f\"Chk:{cnt}\" # Shorter text\n            if cnt >= REQUIRED_FRAMES: \n                color = (0, 0, 255) # Red\n                cv2.putText(frame, \"SNATCH!\", (x1, y1-30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3)\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 4)\n                cv2.circle(frame, neck_center, neck_radius, color, -1)\n            else:\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n                cv2.circle(frame, neck_center, neck_radius, (255, 255, 0), 2)\n            \n            cv2.putText(frame, status, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\nprint(f\"DONE! Saved to {OUTPUT_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Install specific versions to prevent crashes\n!pip install \"numpy==1.26.4\" \"scipy==1.11.4\" \"ultralytics\" \"onnxruntime-gpu\" \"lapx\" --force-reinstall\n\n# 2. Clear confusing warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\" INSTALLATION COMPLETE.\")\nprint(\" CRITICAL STEP: Go to the Menu Bar -> 'Run' -> 'Restart Session'.\")\nprint(\"Then skip this cell and run STEP 2 below.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nimport sys\nimport os\n\n# --- A. PYTORCH SAFETY PATCH (Prevents RecursionError) ---\nif not hasattr(torch.load, \"_is_patched\"):\n    _original_load = torch.load\n    def safe_load_wrapper(*args, **kwargs):\n        if 'weights_only' not in kwargs:\n            kwargs['weights_only'] = False\n        return _original_load(*args, **kwargs)\n    safe_load_wrapper._is_patched = True\n    torch.load = safe_load_wrapper\n\n# --- B. SETUP DEEP OC-SORT TRACKER ---\n# We look for the tracker in the input folder\nsys.path.append('/kaggle/input/using-deep-oc-sort')\n\ntry:\n    from deep_oc_sort.ocsort import OCSort\n    print(\" Deep OC-SORT imported successfully!\")\nexcept ImportError:\n    # If the folder name is slightly different, try a fallback path\n    print(\" Standard import failed. Trying fallback path...\")\n    sys.path.append('/kaggle/input/using-deep-oc-sort/deep_oc_sort')\n    try:\n        from ocsort import OCSort\n        print(\" Deep OC-SORT imported from fallback!\")\n    except:\n        print(\" ERROR: Could not find OCSort. Tracking will fail.\")\n        # Dummy class to prevent code crashing (but tracking won't work)\n        class OCSort:\n            def __init__(self, **kwargs): pass\n            def update(self, dets, img): return dets \n\n# Initialize Tracker (max_age=30 keeps IDs alive for 1 second of occlusion)\ntracker = OCSort(det_thresh=0.4, max_age=30, min_hits=3)\n\n# --- C. CONFIGURATION ---\n# Path to YOUR CUSTOM YOLO MODEL\nYOLO_PATH = '/kaggle/input/best-yolov8m/pytorch/default/1/best _yolov8m.pt'\n# Path to RTMPose (Animal Model)\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx' \n# Path to Video\nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_11.mp4'\nOUTPUT_PATH = '/kaggle/working/output_final_custom_ocsort.mp4'\n\nINPUT_H, INPUT_W = 256, 256\nSNATCH_TIME_THRESHOLD = 0.3 # 0.3s trigger\n\n# --- D. LOAD MODELS ---\nprint(f\"Loading Custom YOLO from: {YOLO_PATH}\")\ntry:\n    yolo_model = YOLO(YOLO_PATH)\nexcept Exception as e:\n    print(f\" Error loading YOLO: {e}\")\n    print(\"Double check the filename (spaces?) or path.\")\n    exit()\n\nprint(\"Loading RTMPose...\")\ntry:\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=providers)\nexcept:\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\ninput_name = ort_session.get_inputs()[0].name\n\n# --- E. HELPER FUNCTIONS ---\ndef get_neck_from_box(x1, y1, x2, y2):\n    box_h, box_w = y2 - y1, x2 - x1\n    # Neck is top 15% of the bounding box\n    return (int(x1 + box_w/2), int(y1 + box_h*0.15)), int(box_h*0.15)\n\ndef is_point_in_circle(p, c, r):\n    return np.sqrt((p[0]-c[0])**2 + (p[1]-c[1])**2) < r\n\ndef preprocess_for_rtmpose(img, h, w):\n    resized = cv2.resize(img, (w, h))\n    mean, std = np.array([123.675, 116.28, 103.53]), np.array([58.395, 57.12, 57.375])\n    img_data = (cv2.cvtColor(resized, cv2.COLOR_BGR2RGB) - mean) / std\n    return np.expand_dims(img_data.astype(np.float32).transpose(2, 0, 1), axis=0)\n\ndef decode_simcc_output(outputs, crop_h, crop_w):\n    simcc_x, simcc_y = outputs[0], outputs[1]\n    x_locs = np.argmax(simcc_x, axis=2)[0] / simcc_x.shape[2] * crop_w\n    y_locs = np.argmax(simcc_y, axis=2)[0] / simcc_y.shape[2] * crop_h\n    return np.stack([x_locs, y_locs], axis=-1)\n\n# --- F. MAIN PROCESSING LOOP ---\nif not os.path.exists(VIDEO_PATH):\n    print(f\" Video not found at {VIDEO_PATH}\")\n    exit()\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nwidth, height = int(cap.get(3)), int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS) or 25\nREQUIRED_FRAMES = int(SNATCH_TIME_THRESHOLD * fps)\n\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(fps), (width, height))\nsnatch_history = {} \n\nprint(f\" Processing... Alarm triggers at {REQUIRED_FRAMES} frames.\")\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    \n    # 1. DETECT (Using Custom YOLO)\n    # We set classes=None to detect whatever your custom model was trained on\n    results = yolo_model.predict(frame, verbose=False, conf=0.4)\n    \n    detections_list = []\n    for r in results:\n        for box in r.boxes:\n            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n            conf = box.conf[0].cpu().numpy()\n            detections_list.append([x1, y1, x2, y2, conf])\n    \n    detections_array = np.array(detections_list)\n    \n    # 2. TRACK (Deep OC-SORT)\n    if len(detections_array) > 0:\n        track_results = tracker.update(detections_array, frame)\n    else:\n        track_results = np.empty((0, 5))\n\n    # 3. ANALYZE BEHAVIOR\n    for track in track_results:\n        x1, y1, x2, y2, box_id = map(int, track[:5])\n        \n        if box_id not in snatch_history: snatch_history[box_id] = 0\n        \n        # Neck Zone\n        neck_center, neck_radius = get_neck_from_box(x1, y1, x2, y2)\n        \n        # Check Hands (Wide Net for Animal Model)\n        hand_in_zone = False\n        crop = frame[max(0,y1):min(height,y2), max(0,x1):min(width,x2)]\n        \n        if crop.size > 0:\n            onnx_input = preprocess_for_rtmpose(crop, INPUT_H, INPUT_W)\n            outputs = ort_session.run(None, {input_name: onnx_input})\n            kpts = decode_simcc_output(outputs, crop.shape[0], crop.shape[1])\n            \n            # Check Indices 5-12 (All potential limb points for Animal Model)\n            for k in range(5, 13):\n                if k < len(kpts):\n                    real_x, real_y = int(kpts[k][0] + x1), int(kpts[k][1] + y1)\n                    if is_point_in_circle((real_x, real_y), neck_center, neck_radius):\n                        hand_in_zone = True\n\n        # Update History\n        if hand_in_zone: snatch_history[box_id] += 1\n        else: snatch_history[box_id] = max(0, snatch_history[box_id] - 1) \n        \n        # Draw Visuals\n        cnt = snatch_history[box_id]\n        color = (0, 255, 0)\n        label = f\"ID:{box_id}\"\n        \n        if cnt > 0: \n            color = (0, 165, 255) # Orange\n            label = f\"ID:{box_id} Chk:{cnt}\"\n        if cnt >= REQUIRED_FRAMES: \n            color = (0, 0, 255) # Red\n            cv2.putText(frame, \"SNATCH!\", (x1, y1-30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3)\n            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 4)\n            cv2.circle(frame, neck_center, neck_radius, color, -1)\n        else:\n            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n            cv2.circle(frame, neck_center, neck_radius, (255, 255, 0), 2)\n        \n        cv2.putText(frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\nprint(f\" DONE! Output saved to: {OUTPUT_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Force reinstall Pillow and dependencies to fix the \"Ink\" error\n!pip install \"pillow>=10.3.0\" \"numpy==1.26.4\" \"scipy==1.11.4\" \"ultralytics\" --force-reinstall","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nimport sys\nimport os\n\n# --- A. PYTORCH SAFETY PATCH ---\nif not hasattr(torch.load, \"_is_patched\"):\n    _original_load = torch.load\n    def safe_load_wrapper(*args, **kwargs):\n        if 'weights_only' not in kwargs:\n            kwargs['weights_only'] = False\n        return _original_load(*args, **kwargs)\n    safe_load_wrapper._is_patched = True\n    torch.load = safe_load_wrapper\n\n# --- B. SETUP DEEP OC-SORT TRACKER ---\nsys.path.append('/kaggle/input/using-deep-oc-sort')\n\ntry:\n    from deep_oc_sort.ocsort import OCSort\n    print(\" Deep OC-SORT imported successfully!\")\nexcept ImportError:\n    print(\" Standard import failed. Trying fallback path...\")\n    sys.path.append('/kaggle/input/using-deep-oc-sort/deep_oc_sort')\n    try:\n        from ocsort import OCSort\n        print(\" Deep OC-SORT imported from fallback!\")\n    except:\n        print(\" ERROR: Could not find OCSort. Tracking will fail.\")\n        class OCSort:\n            def __init__(self, **kwargs): pass\n            def update(self, dets, img): return dets \n\n# Initialize Tracker\ntracker = OCSort(det_thresh=0.4, max_age=30, min_hits=3)\n\n# --- C. CONFIGURATION ---\nYOLO_PATH = '/kaggle/input/best-yolov8m/pytorch/default/1/best _yolov8m.pt'\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx' \nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_11.mp4'\nOUTPUT_PATH = '/kaggle/working/output_final_custom_ocsort.mp4'\n\nINPUT_H, INPUT_W = 256, 256\nSNATCH_TIME_THRESHOLD = 0.3 \n\n# --- D. LOAD MODELS ---\nprint(f\"Loading Custom YOLO from: {YOLO_PATH}\")\ntry:\n    yolo_model = YOLO(YOLO_PATH)\nexcept Exception as e:\n    print(f\" Error loading YOLO: {e}\")\n    exit()\n\nprint(\"Loading RTMPose...\")\ntry:\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=providers)\nexcept:\n    ort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\ninput_name = ort_session.get_inputs()[0].name\n\n# --- E. HELPER FUNCTIONS ---\ndef get_neck_from_box(x1, y1, x2, y2):\n    box_h, box_w = y2 - y1, x2 - x1\n    return (int(x1 + box_w/2), int(y1 + box_h*0.15)), int(box_h*0.15)\n\ndef is_point_in_circle(p, c, r):\n    return np.sqrt((p[0]-c[0])**2 + (p[1]-c[1])**2) < r\n\ndef preprocess_for_rtmpose(img, h, w):\n    resized = cv2.resize(img, (w, h))\n    mean, std = np.array([123.675, 116.28, 103.53]), np.array([58.395, 57.12, 57.375])\n    img_data = (cv2.cvtColor(resized, cv2.COLOR_BGR2RGB) - mean) / std\n    return np.expand_dims(img_data.astype(np.float32).transpose(2, 0, 1), axis=0)\n\ndef decode_simcc_output(outputs, crop_h, crop_w):\n    simcc_x, simcc_y = outputs[0], outputs[1]\n    x_locs = np.argmax(simcc_x, axis=2)[0] / simcc_x.shape[2] * crop_w\n    y_locs = np.argmax(simcc_y, axis=2)[0] / simcc_y.shape[2] * crop_h\n    return np.stack([x_locs, y_locs], axis=-1)\n\n# --- F. MAIN PROCESSING LOOP ---\nif not os.path.exists(VIDEO_PATH):\n    print(f\" Video not found at {VIDEO_PATH}\")\n    exit()\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nwidth, height = int(cap.get(3)), int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS) or 25\nREQUIRED_FRAMES = int(SNATCH_TIME_THRESHOLD * fps)\n\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(fps), (width, height))\nsnatch_history = {} \n\nprint(f\" Processing... Alarm triggers at {REQUIRED_FRAMES} frames.\")\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    \n    # 1. DETECT (Custom YOLO)\n    results = yolo_model.predict(frame, verbose=False, conf=0.4)\n    \n    detections_list = []\n    for r in results:\n        for box in r.boxes:\n            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n            conf = box.conf[0].cpu().numpy()\n            detections_list.append([x1, y1, x2, y2, conf])\n    \n    detections_array = np.array(detections_list)\n    \n    # 2. TRACK (Deep OC-SORT)\n    if len(detections_array) > 0:\n        track_results = tracker.update(detections_array, frame)\n    else:\n        track_results = np.empty((0, 5))\n\n    # 3. ANALYZE\n    for track in track_results:\n        x1, y1, x2, y2, box_id = map(int, track[:5])\n        \n        if box_id not in snatch_history: snatch_history[box_id] = 0\n        \n        # Neck Zone\n        neck_center, neck_radius = get_neck_from_box(x1, y1, x2, y2)\n        \n        # Check Hands (Wide Net for Animal Model)\n        hand_in_zone = False\n        crop = frame[max(0,y1):min(height,y2), max(0,x1):min(width,x2)]\n        \n        if crop.size > 0:\n            onnx_input = preprocess_for_rtmpose(crop, INPUT_H, INPUT_W)\n            outputs = ort_session.run(None, {input_name: onnx_input})\n            kpts = decode_simcc_output(outputs, crop.shape[0], crop.shape[1])\n            \n            # Check Indices 5-12\n            for k in range(5, 13):\n                if k < len(kpts):\n                    real_x, real_y = int(kpts[k][0] + x1), int(kpts[k][1] + y1)\n                    if is_point_in_circle((real_x, real_y), neck_center, neck_radius):\n                        hand_in_zone = True\n\n        # Update History\n        if hand_in_zone: snatch_history[box_id] += 1\n        else: snatch_history[box_id] = max(0, snatch_history[box_id] - 1) \n        \n        # Draw Visuals\n        cnt = snatch_history[box_id]\n        color = (0, 255, 0)\n        label = f\"ID:{box_id}\"\n        \n        if cnt > 0: \n            color = (0, 165, 255) # Orange\n            label = f\"ID:{box_id} Chk:{cnt}\"\n        if cnt >= REQUIRED_FRAMES: \n            color = (0, 0, 255) # Red\n            cv2.putText(frame, \"SNATCH!\", (x1, y1-30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3)\n            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 4)\n            cv2.circle(frame, neck_center, neck_radius, color, -1)\n        else:\n            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n            cv2.circle(frame, neck_center, neck_radius, (255, 255, 0), 2)\n        \n        cv2.putText(frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\nprint(f\" DONE! Output saved to: {OUTPUT_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nimport sys\nimport os\n\n# --- A. FIX OCSORT IMPORT (Auto-Search) ---\n# This block hunts for 'ocsort.py' instead of guessing the path\nprint(\" Searching for Deep OC-SORT...\")\ndataset_root = '/kaggle/input/using-deep-oc-sort'\nfound_tracker = False\n\nfor root, dirs, files in os.walk(dataset_root):\n    if 'ocsort.py' in files:\n        sys.path.append(root)  # Add the folder containing the file\n        sys.path.append(os.path.dirname(root)) # Add the parent folder\n        print(f\" FOUND tracker in: {root}\")\n        found_tracker = True\n        try:\n            from ocsort import OCSort\n            print(\" Import Successful!\")\n            break\n        except ImportError:\n            try:\n                # Try importing as a package\n                from deep_oc_sort.ocsort import OCSort\n                print(\" Package Import Successful!\")\n                break\n            except:\n                pass\n\nif not found_tracker:\n    print(\" CRITICAL ERROR: Could not find 'ocsort.py'.\")\n    print(\"Listing files in dataset to help debug:\")\n    for root, dirs, files in os.walk(dataset_root):\n        print(f\"  {root} -> {files}\")\n    # Dummy backup to prevent crash (but tracking won't work)\n    class OCSort:\n        def __init__(self, **kwargs): pass\n        def update(self, dets, img): return dets\n\n# Initialize Tracker\ntracker = OCSort(det_thresh=0.4, max_age=30, min_hits=3)\n\n# --- B. CONFIGURATION ---\nYOLO_PATH = '/kaggle/input/best-yolov8m/pytorch/default/1/best _yolov8m.pt'\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx' \nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_11.mp4'\nOUTPUT_PATH = '/kaggle/working/output_final_fixed.mp4'\n\nINPUT_H, INPUT_W = 256, 256\nSNATCH_TIME_THRESHOLD = 0.3 \n\n# --- C. LOAD MODELS (CPU Forced to stop errors) ---\nprint(f\"Loading Custom YOLO...\")\nyolo_model = YOLO(YOLO_PATH)\n\nprint(\"Loading RTMPose (CPU Mode)...\")\n# We explicitly use CPU to stop the \"CUDA failure 35\" error log\nort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\ninput_name = ort_session.get_inputs()[0].name\n\n# --- D. HELPER FUNCTIONS ---\ndef get_neck_from_box(x1, y1, x2, y2):\n    box_h, box_w = y2 - y1, x2 - x1\n    return (int(x1 + box_w/2), int(y1 + box_h*0.15)), int(box_h*0.15)\n\ndef is_point_in_circle(p, c, r):\n    return np.sqrt((p[0]-c[0])**2 + (p[1]-c[1])**2) < r\n\ndef preprocess_for_rtmpose(img, h, w):\n    resized = cv2.resize(img, (w, h))\n    mean, std = np.array([123.675, 116.28, 103.53]), np.array([58.395, 57.12, 57.375])\n    img_data = (cv2.cvtColor(resized, cv2.COLOR_BGR2RGB) - mean) / std\n    return np.expand_dims(img_data.astype(np.float32).transpose(2, 0, 1), axis=0)\n\ndef decode_simcc_output(outputs, crop_h, crop_w):\n    simcc_x, simcc_y = outputs[0], outputs[1]\n    x_locs = np.argmax(simcc_x, axis=2)[0] / simcc_x.shape[2] * crop_w\n    y_locs = np.argmax(simcc_y, axis=2)[0] / simcc_y.shape[2] * crop_h\n    return np.stack([x_locs, y_locs], axis=-1)\n\n# --- E. MAIN LOOP ---\nif not os.path.exists(VIDEO_PATH):\n    print(f\" Video not found at {VIDEO_PATH}\")\n    exit()\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nwidth, height = int(cap.get(3)), int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS) or 25\nREQUIRED_FRAMES = int(SNATCH_TIME_THRESHOLD * fps)\n\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(fps), (width, height))\nsnatch_history = {} \n\nprint(f\" Processing... Alarm triggers at {REQUIRED_FRAMES} frames.\")\n\nframe_idx = 0\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    frame_idx += 1\n    \n    # 1. DETECT (Custom YOLO)\n    results = yolo_model.predict(frame, verbose=False, conf=0.4)\n    \n    detections_list = []\n    for r in results:\n        for box in r.boxes:\n            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n            conf = box.conf[0].cpu().numpy()\n            detections_list.append([x1, y1, x2, y2, conf])\n    \n    detections_array = np.array(detections_list)\n    \n    # 2. TRACK (Deep OC-SORT)\n    if len(detections_array) > 0:\n        track_results = tracker.update(detections_array, frame)\n    else:\n        track_results = np.empty((0, 5))\n\n    # 3. ANALYZE\n    for track in track_results:\n        x1, y1, x2, y2, box_id = map(int, track[:5])\n        \n        if box_id not in snatch_history: snatch_history[box_id] = 0\n        \n        # Neck Zone\n        neck_center, neck_radius = get_neck_from_box(x1, y1, x2, y2)\n        \n        # Check Hands\n        hand_in_zone = False\n        crop = frame[max(0,y1):min(height,y2), max(0,x1):min(width,x2)]\n        \n        if crop.size > 0:\n            onnx_input = preprocess_for_rtmpose(crop, INPUT_H, INPUT_W)\n            outputs = ort_session.run(None, {input_name: onnx_input})\n            kpts = decode_simcc_output(outputs, crop.shape[0], crop.shape[1])\n            \n            # Check Paws/Hands\n            for k in range(5, 13):\n                if k < len(kpts):\n                    real_x, real_y = int(kpts[k][0] + x1), int(kpts[k][1] + y1)\n                    if is_point_in_circle((real_x, real_y), neck_center, neck_radius):\n                        hand_in_zone = True\n\n        # Update\n        if hand_in_zone: snatch_history[box_id] += 1\n        else: snatch_history[box_id] = max(0, snatch_history[box_id] - 1) \n        \n        # Draw\n        cnt = snatch_history[box_id]\n        color = (0, 255, 0)\n        label = f\"ID:{box_id}\"\n        \n        if cnt > 0: \n            color = (0, 165, 255)\n            label = f\"ID:{box_id} Chk:{cnt}\"\n        if cnt >= REQUIRED_FRAMES: \n            color = (0, 0, 255)\n            cv2.putText(frame, \"SNATCH!\", (x1, y1-30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3)\n            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 4)\n            cv2.circle(frame, neck_center, neck_radius, color, -1)\n        else:\n            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n            cv2.circle(frame, neck_center, neck_radius, (255, 255, 0), 2)\n        \n        cv2.putText(frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    out.write(frame)\n    if frame_idx % 20 == 0: print(f\"Processing frame {frame_idx}\")\n\ncap.release()\nout.release()\nprint(f\" DONE! Output saved to: {OUTPUT_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install \"onnxruntime-gpu\" \"ultralytics\" \"numpy==1.26.4\" \"scipy==1.11.4\" \"pillow>=10.3.0\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nimport os\n\n# --- A. CONFIGURATION ---\n# 1. NEW PATH (YOLO11 Small)\nYOLO_PATH = '/kaggle/input/yolo-11/yolo11s.pt'\n\n# 2. POSE MODEL (Animal Model \"Hacked\" for Humans)\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx' \n\n# 3. VIDEO PATH\nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_1.mp4'\nOUTPUT_PATH = '/kaggle/working/final_output_yolo11_bytetrack.mp4'\n\n# 4. SETTINGS\nINPUT_H, INPUT_W = 256, 256\nSNATCH_TIME_THRESHOLD = 0.3  # 0.3s trigger\n\n# --- B. LOAD MODELS ---\nprint(f\"Loading YOLO11 Model: {YOLO_PATH}...\")\ntry:\n    yolo_model = YOLO(YOLO_PATH)\nexcept Exception as e:\n    print(f\" Error loading YOLO: {e}\")\n    exit()\n\nprint(\"Loading Pose Model (CPU)...\")\n# Force CPU to avoid CUDA errors\nort_session = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\ninput_name = ort_session.get_inputs()[0].name\n\n# --- C. HELPER FUNCTIONS ---\ndef get_neck_from_box(x1, y1, x2, y2):\n    # Neck is roughly top 15% of the bounding box\n    box_h, box_w = y2 - y1, x2 - x1\n    return (int(x1 + box_w/2), int(y1 + box_h*0.15)), int(box_h*0.15)\n\ndef is_point_in_circle(p, c, r):\n    return np.sqrt((p[0]-c[0])**2 + (p[1]-c[1])**2) < r\n\ndef preprocess_for_rtmpose(img, h, w):\n    resized = cv2.resize(img, (w, h))\n    mean, std = np.array([123.675, 116.28, 103.53]), np.array([58.395, 57.12, 57.375])\n    img_data = (cv2.cvtColor(resized, cv2.COLOR_BGR2RGB) - mean) / std\n    return np.expand_dims(img_data.astype(np.float32).transpose(2, 0, 1), axis=0)\n\ndef decode_simcc_output(outputs, crop_h, crop_w):\n    simcc_x, simcc_y = outputs[0], outputs[1]\n    x_locs = np.argmax(simcc_x, axis=2)[0] / simcc_x.shape[2] * crop_w\n    y_locs = np.argmax(simcc_y, axis=2)[0] / simcc_y.shape[2] * crop_h\n    return np.stack([x_locs, y_locs], axis=-1)\n\n# --- D. MAIN PROCESSING LOOP ---\nif not os.path.exists(VIDEO_PATH):\n    print(f\" Video not found at {VIDEO_PATH}\")\n    exit()\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nwidth, height = int(cap.get(3)), int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS) or 25\nREQUIRED_FRAMES = int(SNATCH_TIME_THRESHOLD * fps)\n\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(fps), (width, height))\nsnatch_history = {} \n\nprint(f\" Processing with YOLO11 + ByteTrack...\")\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    \n    # 1. TRACK (Using Built-in Ultralytics ByteTrack)\n    # This replaces the entire old Deep OC-SORT block\n    results = yolo_model.track(\n        frame, \n        persist=True, \n        tracker=\"bytetrack.yaml\",  # <--- HERE IS THE BYTETRACK IMPORT\n        verbose=False,\n        classes=[0] # Only track class 0 (Person)\n    )\n    \n    # 2. ANALYZE RESULTS\n    # Ultralytics handles the tracking internally now\n    for r in results:\n        if r.boxes.id is None: continue # Skip if no ID assigned yet\n        \n        boxes = r.boxes.xyxy.cpu().numpy()\n        ids = r.boxes.id.cpu().numpy()\n        \n        for box, box_id in zip(boxes, ids):\n            x1, y1, x2, y2 = map(int, box[:4])\n            box_id = int(box_id)\n            \n            if box_id not in snatch_history: snatch_history[box_id] = 0\n            \n            # --- SNATCHING LOGIC STARTS HERE ---\n            \n            # A. Define Neck Zone\n            neck_center, neck_radius = get_neck_from_box(x1, y1, x2, y2)\n            \n            # B. Check Hands (Pose Model)\n            hand_in_zone = False\n            crop = frame[max(0,y1):min(height,y2), max(0,x1):min(width,x2)]\n            \n            if crop.size > 0:\n                onnx_input = preprocess_for_rtmpose(crop, INPUT_H, INPUT_W)\n                outputs = ort_session.run(None, {input_name: onnx_input})\n                kpts = decode_simcc_output(outputs, crop.shape[0], crop.shape[1])\n                \n                # Check indices 5-12 (Paws/Elbows mapped to Hands)\n                for k in range(5, 13):\n                    if k < len(kpts):\n                        real_x, real_y = int(kpts[k][0] + x1), int(kpts[k][1] + y1)\n                        if is_point_in_circle((real_x, real_y), neck_center, neck_radius):\n                            hand_in_zone = True\n\n            # C. Update History\n            if hand_in_zone: snatch_history[box_id] += 1\n            else: snatch_history[box_id] = max(0, snatch_history[box_id] - 1) \n            \n            # D. Draw Visuals\n            cnt = snatch_history[box_id]\n            color = (0, 255, 0) # Green\n            label = f\"ID:{box_id}\"\n            \n            if cnt > 0: \n                color = (0, 165, 255) # Orange\n                label = f\"ID:{box_id} Chk:{cnt}\"\n            if cnt >= REQUIRED_FRAMES: \n                color = (0, 0, 255) # Red (Alarm)\n                cv2.putText(frame, \"SNATCH!\", (x1, y1-30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3)\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 4)\n                cv2.circle(frame, neck_center, neck_radius, color, -1)\n            else:\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n                cv2.circle(frame, neck_center, neck_radius, (255, 255, 0), 2)\n            \n            cv2.putText(frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\nprint(f\" DONE! Video saved to: {OUTPUT_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T08:32:14.068173Z","iopub.execute_input":"2025-12-29T08:32:14.068531Z","iopub.status.idle":"2025-12-29T08:35:10.551068Z","shell.execute_reply.started":"2025-12-29T08:32:14.068499Z","shell.execute_reply":"2025-12-29T08:35:10.549094Z"}},"outputs":[{"name":"stdout","text":"Loading YOLO11 Model: /kaggle/input/yolo-11/yolo11s.pt...\nLoading Pose Model (CPU)...\n Processing with YOLO11 + ByteTrack...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_202/1879145744.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0monnx_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_for_rtmpose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINPUT_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINPUT_W\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mort_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0monnx_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m                 \u001b[0mkpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_simcc_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1}]}