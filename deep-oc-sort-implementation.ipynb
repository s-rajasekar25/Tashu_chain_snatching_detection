{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13957334,"sourceType":"datasetVersion","datasetId":8896695},{"sourceId":695356,"sourceType":"modelInstanceVersion","modelInstanceId":527360,"modelId":541406},{"sourceId":696513,"sourceType":"modelInstanceVersion","modelInstanceId":528289,"modelId":542336},{"sourceId":724957,"sourceType":"modelInstanceVersion","modelInstanceId":551735,"modelId":564315}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport subprocess\n\nprint(\"ðŸ”„ Installing clean libraries...\")\n\n# 1. Uninstall broken packages\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"numpy\", \"opencv-python-headless\", \"ultralytics\"], stdout=subprocess.DEVNULL)\n\n# 2. Install compatible versions (Pinned for stability)\n# numpy<2.0 is CRITICAL to prevent the error you just saw\ninstall_cmd = [\n    sys.executable, \"-m\", \"pip\", \"install\", \n    \"numpy<2.0\", \n    \"ultralytics\", \n    \"onnxruntime-gpu\",\n    \"opencv-python-headless\", # Ensures CV2 works\n    \"--no-cache-dir\"\n]\nsubprocess.run(install_cmd, stdout=subprocess.DEVNULL)\n\n# 3. Reload environment\nimport site\nimport importlib\nimportlib.reload(site)\n\nprint(\"âœ… Installation Complete. You can now run the tracker.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport subprocess\n\nprint(\"â¬‡ï¸ Installing ALL required modules at once...\")\n\n# This list covers every dependency boxmot needs to avoid future errors\nrequirements = [\n    \"filterpy\",          # The error you just saw\n    \"lapx\",              # Linear Assignment\n    \"yacs\",              # Config helper\n    \"ftfy\",              # Text fixer\n    \"regex\",             # Text helper\n    \"loguru\",            # Logging\n    \"thop\",              # Performance counter\n    \"scikit-image\",      # Image processing\n    \"gdown\",             # File downloader\n    \"onnxruntime-gpu\",   # Model acceleration\n    \"ultralytics\",       # YOLO\n    \"numpy<2.0\"          # Critical version lock\n]\n\n# Install everything in one command\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\"] + requirements, stdout=subprocess.DEVNULL)\n\nprint(\"âœ… ALL MODULES INSTALLED. The code below will work now.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport subprocess\nimport os\n\nprint(\"â¬‡ï¸ FIXING BOXMOT VERSION...\")\n\n# 1. Uninstall the broken/random version\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"boxmot\"], stdout=subprocess.DEVNULL)\n\n# 2. Install specific stable version (10.0.43) + Dependencies\n# We install the dependencies manually first to avoid conflicts\ndeps = [\"filterpy\", \"lapx\", \"yacs\", \"ftfy\", \"regex\", \"loguru\", \"thop\", \"termcolor\", \"scikit-image\", \"gdown\"]\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\"] + deps, stdout=subprocess.DEVNULL)\n\n# 3. Install the specific BoxMOT version\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"boxmot==10.0.43\", \"--no-deps\"], stdout=subprocess.DEVNULL)\n\nprint(\"âœ… Version Fixed. Now run the main code.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport onnxruntime as ort\nfrom ultralytics import YOLO\nfrom pathlib import Path\nimport torch\nimport os\nimport sys\n\n# Import Deep OC-SORT\ntry:\n    from boxmot import DeepOCSORT\nexcept ImportError:\n    from boxmot.trackers.deepocsort.deep_oc_sort import DeepOCSORT\n\n# ==========================================\n# 1. DEBUG SETTINGS (High Sensitivity)\n# ==========================================\nCONF_THRESH = 0.20        # Lower confidence to see more\nASPECT_RATIO_MAX = 0.9\nMIN_AREA = 2000\n\n# -- DEBUG TUNING --\nNECK_ZONE_SIZE = 0.20     # Wider zone (0.20 instead of 0.15)\nMIN_SNATCH_FRAMES = 1     # ðŸš¨ INSTANT TRIGGER (For testing)\nMAX_SNATCH_FRAMES = 100   \n\n# ==========================================\n# 2. SETUP\n# ==========================================\ntracker = DeepOCSORT(\n    model_weights=Path('osnet_x0_25_msmt17.pt'),\n    device='cuda' if torch.cuda.is_available() else 'cpu',\n    fp16=False,\n    det_thresh=CONF_THRESH\n)\n\ndef preprocess_pose(img):\n    resized = cv2.resize(img, (256, 256))\n    mean, std = np.array([123.675, 116.28, 103.53]), np.array([58.395, 57.12, 57.375])\n    return np.expand_dims(((cv2.cvtColor(resized, cv2.COLOR_BGR2RGB) - mean) / std).astype(np.float32).transpose(2, 0, 1), axis=0)\n\ndef get_kpts(outputs, h, w):\n    simcc_x, simcc_y = outputs[0], outputs[1]\n    x = np.argmax(simcc_x, axis=2)[0] / simcc_x.shape[2] * w\n    y = np.argmax(simcc_y, axis=2)[0] / simcc_y.shape[2] * h\n    return np.stack([x, y], axis=-1)\n\n# ==========================================\n# 3. MAIN LOOP\n# ==========================================\nYOLO_PATH = '/kaggle/input/best-yolov8m/pytorch/default/1/best _yolov8m.pt'\nRTMPOSE_ONNX_PATH = '/kaggle/input/rtm-pose/onnx/default/1/rtmpose-m_ap10k_256.onnx' \nVIDEO_PATH = '/kaggle/input/snatching-videos/snatching_videos/snatching_18.mp4'\nOUTPUT_PATH = '/kaggle/working/debug_output.mp4'\n\nif not os.path.exists(VIDEO_PATH):\n    print(\"âŒ Video missing.\")\n    sys.exit()\n\ntry:\n    yolo = YOLO(YOLO_PATH)\nexcept:\n    yolo = YOLO('yolo11s.pt')\n\nort_sess = ort.InferenceSession(RTMPOSE_ONNX_PATH, providers=['CPUExecutionProvider'])\nin_name = ort_sess.get_inputs()[0].name\n\ncap = cv2.VideoCapture(VIDEO_PATH)\nW, H = int(cap.get(3)), int(cap.get(4))\nFPS = cap.get(cv2.CAP_PROP_FPS) or 25\nout = cv2.VideoWriter(OUTPUT_PATH, cv2.VideoWriter_fourcc(*'mp4v'), int(FPS), (W, H))\n\ninteraction_timers = {}\n\nprint(\"ðŸš€ Running DEBUG MODE (X-Ray)...\")\n\nframe_count = 0\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret: break\n    frame_count += 1\n    if frame_count % 20 == 0: print(f\"Processing frame {frame_count}...\", end='\\r')\n    \n    # 1. DETECT\n    results = yolo.predict(frame, conf=CONF_THRESH, verbose=False, classes=[0])\n    dets_to_track = []\n    \n    for r in results:\n        boxes = r.boxes.xyxy.cpu().numpy()\n        confs = r.boxes.conf.cpu().numpy()\n        clss = r.boxes.cls.cpu().numpy()\n        for box, conf, cls in zip(boxes, confs, clss):\n            x1, y1, x2, y2 = map(int, box[:4])\n            w, h = (x2-x1), (y2-y1)\n            if (w*h) < MIN_AREA: continue\n            if (w / h) > ASPECT_RATIO_MAX: continue\n            dets_to_track.append([x1, y1, x2, y2, conf, cls])\n    \n    dets_to_track = np.array(dets_to_track)\n    tracks = tracker.update(dets_to_track, frame) if len(dets_to_track) > 0 else []\n    \n    # 2. COLLECT DATA\n    people = []\n    for track in tracks:\n        x1, y1, x2, y2 = map(int, track[:4])\n        tid = int(track[4])\n        \n        # ZONE GEOMETRY\n        neck_cx = int(x1 + (x2-x1)//2)\n        neck_cy = int(y1 + (y2-y1)*0.15)\n        neck_rad = int((y2-y1)*NECK_ZONE_SIZE)\n        \n        hands = []\n        shoulders = []\n        crop = frame[max(0,y1):min(H,y2), max(0,x1):min(W,x2)]\n        if crop.size > 0:\n            pose_in = preprocess_pose(crop)\n            kpts = get_kpts(ort_sess.run(None, {in_name: pose_in}), crop.shape[0], crop.shape[1])\n            \n            # Draw Skeleton for Debugging\n            for k in [5,6,7,8,9,10]: # Shoulders, Elbows, Wrists\n                if k < len(kpts):\n                    px, py = int(kpts[k][0]+x1), int(kpts[k][1]+y1)\n                    cv2.circle(frame, (px, py), 3, (0, 255, 255), -1) # Yellow dots\n            \n            # Store Hands (9, 10)\n            for k in [9, 10]:\n                if k < len(kpts):\n                    hands.append((int(kpts[k][0]+x1), int(kpts[k][1]+y1)))\n                    \n        people.append({'id': tid, 'box': (x1,y1,x2,y2), 'neck': (neck_cx,neck_cy,neck_rad), 'hands': hands})\n\n    # 3. INTERACTION CHECK\n    snatch_alerts = set()\n    \n    for p1 in people: # Thief?\n        for p2 in people: # Victim?\n            if p1['id'] == p2['id']: continue\n            \n            pair_key = (p1['id'], p2['id'])\n            vx, vy, vrad = p2['neck']\n            \n            # DEBUG: Draw Neck Zone on everyone (Blue Circle)\n            cv2.circle(frame, (vx, vy), vrad, (255, 0, 0), 1)\n            \n            is_touching = False\n            min_dist = 9999\n            \n            for hx, hy in p1['hands']:\n                # DEBUG: Draw large Red Dot on Hands\n                cv2.circle(frame, (hx, hy), 5, (0, 0, 255), -1)\n                \n                dist = np.sqrt((hx-vx)**2 + (hy-vy)**2)\n                if dist < min_dist: min_dist = dist\n                \n                if dist < vrad:\n                    is_touching = True\n                    # Draw Line showing the contact\n                    cv2.line(frame, (hx, hy), (vx, vy), (0, 0, 255), 2)\n                    cv2.putText(frame, f\"{int(dist)}px\", (hx, hy-10), 0, 0.5, (0,255,255), 1)\n\n            if is_touching:\n                if pair_key not in interaction_timers: interaction_timers[pair_key] = 0\n                interaction_timers[pair_key] += 1\n                \n                frames = interaction_timers[pair_key]\n                if frames >= MIN_SNATCH_FRAMES and frames <= MAX_SNATCH_FRAMES:\n                    snatch_alerts.add(p1['id'])\n                    snatch_alerts.add(p2['id'])\n            else:\n                if pair_key in interaction_timers:\n                    interaction_timers[pair_key] = max(0, interaction_timers[pair_key] - 1)\n                    if interaction_timers[pair_key] == 0: del interaction_timers[pair_key]\n\n    # 4. DRAW\n    for p in people:\n        tid = p['id']\n        x1, y1, x2, y2 = p['box']\n        \n        color = (0, 255, 0)\n        if tid in snatch_alerts:\n            color = (0, 0, 255)\n            cv2.putText(frame, \"SNATCH DETECTED!\", (x1, y1-30), 0, 0.8, color, 2)\n            nx, ny, nr = p['neck']\n            cv2.circle(frame, (nx, ny), nr, color, 3) # Thick Red Circle\n            \n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        cv2.putText(frame, f\"ID:{tid}\", (x1, y1-5), 0, 0.6, (255,255,0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\nprint(f\"âœ… DONE: {OUTPUT_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}